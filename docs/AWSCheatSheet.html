<div class="content post__content clearfix"><p>If you are planning or preparing for AWS Certified Solutions Architect Associate (SAA-C02) exam then this article is for you to get started.</p><h2 id="overview">Overview</h2><hr><h3 id="faqs">FAQs</h3><ol><li>Prepare well for the exam, it is the toughest exam I <a href="https://www.credly.com/badges/9608f5b4-850a-42ec-99b2-a465753f187c/public_url">cracked</a> in recent years.</li><li>Requires <strong>2 to 3 month</strong> of preparation depending upon your commitment per day.</li><li>Exam code is <strong>SAA-C03</strong> (third version) and cost you <strong>150 USD</strong> per attempt.</li><li>You need to solve <strong>65 questions</strong> in <strong>130 mins</strong> from your laptop under the supervision of online proctor.</li><li>Passing score is <strong>720 (out of 1000)</strong> means you should answer at least 47 (out of 65) questions correctly. No negative scoring so answer all the questions!</li><li>You get the result (Pass or Fail) once you submit the exam, however you don’t receive any email immediately. It generally takes 2-3 days. I received an email with digital certificate, score-card and badge after two days. You can also login to <a href="https://www.aws.training/Certification">AWS training</a> to get them later.</li><li>You can <a href="">schedule</a> exam with Pearson VUE or PSI. I heard bad reviews about PSI and chose Pearson VUE for my exam. The exam went smooth.</li><li>You get discount vouchers under Benefits tab of <a href="https://www.aws.training/Certification">AWS training</a> portal once you crack at least one AWS exam. You can use these vouchers for subsequent exams.</li><li><a href="https://d1.awsstatic.com/training-and-certification/docs-sa-assoc/AWS-Certified-Solutions-Architect-Associate_Exam-Guide.pdf">Exam Guide</a> for more details.</li></ol><br><h3 id="learning-path">Learning Path</h3><p>I followed these four steps for the preparation of AWS exam:-</p><h5 id="1-watch-videos">1. Watch Videos</h5><p>First step to your learning path is to go through AWS lecture and training videos, which is easiest way to get familiar with AWS Services. It might take 1-2 months to cover all the AWS services depending upon your daily commitment. I recommend the following lecture videos:-</p><ol><li><a href="https://acloudguru.com/course/aws-certified-solutions-architect-associate-saa-c03">CloudGuru</a> - 35+ hours videos by <strong>Ryan Kroonenburg</strong> with course quizzes, Hands-on labs, 1 practice exam. They also provide AWS Sandbox for unlimited Hands-on when you buy subscription.</li><li><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/">Udemy</a> - 26+ hours videos by <strong>Stephane Maarek</strong> with course quizzes, Hands-on labs, 1 practice exam.</li><li><a href="https://youtu.be/Ia-UEYYR44s">FreeCodeChamp</a> - 10+ hours of amazing video on Youtube, absolutely free!</li></ol><p><strong>Hands-on AWS Services</strong> is very important to visualize AWS services and retain your AWS learning for a long time.</p><h5 id="2-practice-exam">2. Practice Exam</h5><p>Watching videos are not enough! You <strong>must solve</strong> as many practice exams as you can. They gives you a very fair understanding of what to expect in real exam. I recommend the following practice exams:-</p><ol><li><a href="https://www.whizlabs.com/aws-solutions-architect-associate/">Whizlabs</a> - 7 practice tests (65 questions each) and also has topic-wise practice tests.</li><li><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate-amazon-practice-exams-saa-c02/">Udemy</a> - 6 practice tests (65 questions each) by <strong>John Bonso</strong></li><li><a href="https://d1.awsstatic.com/training-and-certification/docs-sa-assoc/AWS-Certified-Solutions-Architect-Associate_Sample-Questions.pdf">AWS - Sample Questions</a> - 10 sample questions</li></ol><h5 id="3-next-step">3. Next Step</h5><p>You can read the following to build confidence:-</p><ol><li><a href="https://aws.amazon.com/faqs/">AWS services FAQs</a> - You will find the answers in FAQs for most of the questions</li><li><a href="https://start.jcolemorrison.com/aws-vpc-core-concepts-analogy-guide/">VPC Analogy</a> - Interesting read to understand the difficult topic VPC &amp; Networking</li><li><a href="https://aws.amazon.com/architecture/well-architected/?ep=sec&amp;sec=assoc_saa&amp;wa-lens-whitepapers.sort-by=item.additionalFields.sortDate&amp;wa-lens-whitepapers.sort-order=desc">AWS Well-Architected White papers</a></li><li><a href="https://d1.awsstatic.com/training-and-certification/ramp-up_guides/Ramp-Up_Guide_Architect.pdf">AWS Ramp-Up Guide: Architect</a></li></ol><h5 id="4-last-step">4. Last Step</h5><p>Once you are done with your preparation and ready for the exam, go through the below exam notes for your last day of preparation:-</p><p>These AWS certification exam notes are the result of watching 50+ hours of AWS training videos, solving 1000+ AWS exam questions, reading AWS services FAQs and White papers. Best of luck with your exam preparation!</p><h2 id="aws-infrastructure">AWS Infrastructure</h2><hr><h3 id="aws-region">AWS Region</h3><ul><li>AWS regions are physical locations around the world having a cluster of data centers.<pre tabindex="0"><code>  | AWS Region                | Code           |
  |---------------------------|----------------|
  | US East (N. Virginia)     | us-east-1      |
  | US East (Ohio)            | us-east-2      | 
  | US West (N. California)   | us-west-1      |
  | US West (Oregon)          | us-west-2      | 
  | Africa (Cape Town)        | af-south-1     |
  | Asia Pacific (Hong Kong)  | ap-east-1      |
  | Asia Pacific (Mumbai)     | ap-south-1     |
  | Asia Pacific (Osaka)      | ap-northeast-3 |
  | Asia Pacific (Seoul)      | ap-northeast-2 |
  | Asia Pacific (Singapore)  | ap-southeast-1 |
  | Asia Pacific (Sydney)     | ap-southeast-2 |
  | Asia Pacific (Tokyo)      | ap-northeast-1 |
  | Canada (Central)          | ca-central-1   |
  | Europe (Frankfurt)        | eu-central-1   |
  | Europe (Ireland)          | eu-west-1      |
  | Europe (London)           | eu-west-2      |
  | Europe (Milan)            | eu-south-1     |
  | Europe (Paris)            | eu-west-3      |
  | Europe (Stockholm)        | eu-north-1     |
  | Middle East (Bahrain)     | me-south-1     |
  | South America (São Paulo) | sa-east-1      |
</code></pre></li><li>You need to select the region first for most of the AWS services such as EC2, ELB, S3, Lambda, etc.</li><li>You can not select region for Global AWS services such as IAM, AWS Organizations, Route 53, CloudFront, WAF, etc.</li><li>Each AWS Region consists of multiple, isolated, and physically separate <strong>AZs (Availability Zones)</strong> within a geographic area.</li></ul><br><h3 id="az-availability-zones">AZ (Availability zones)</h3><ul><li>An AZ is one or more discrete data centers with redundant power, networking, and connectivity</li><li>All AZs in an AWS Region are interconnected with high-bandwidth, low-latency networking.</li><li>Customer deploy applications across multiple AZs in same region for high-availability, scalability, fault-tolerant and low-latency.</li><li>AZs in a region are usually 3, min is 2 and max is 6 for e.g. <strong>3 AZs in Ohio are us-east-2a, us-east-2b, and us-east-2c</strong>.</li><li>For high availability in us-east-2 region with min 6 instances required <strong>either</strong> place 3 instances in each 3 AZs <strong>or</strong> place 6 instances in each 2 AZs (choose any 2 AZs out of 3) so that it works normal when 1 AZ goes down.</li></ul><h2 id="security-identity--compliance">Security, Identity &amp; Compliance</h2><hr><h3 id="iam-identity-and-access-management">IAM (Identity and Access Management)</h3><ul><li>IAM is used to manage <strong>access</strong> to users and resources</li><li>IAM is a global service (applied to all the regions at the same time). IAM is a free service.</li><li><strong>Root account</strong> is created by default with full administrator, shouldn’t be used</li><li><strong>Users</strong> mapped to physical user, should login to AWS console with their own account and password</li><li><strong>Groups</strong> can have one or more users, can not have other groups</li><li><strong>Policies</strong> are JSON documents that <strong>Allow or Deny</strong> the access on <strong>action</strong> can be performed on AWS <strong>resource</strong> by any user, group and role<ul><li><strong>Version</strong> policy language version. 2012-10-17 is latest version.</li><li><strong>Statement</strong> container for one or more policy statements</li><li><strong>Sid</strong> (optional) a way of labeling your policy statement</li><li><strong>Effect</strong> set whether the policy Allow or Deny</li><li><strong>Principal</strong> user, group, role, or federated user to which you would like to allow or deny access</li><li><strong>Action</strong> one or more actions that can be performed on AWS resources</li><li><strong>Resource</strong> one or more AWS resource to which actions apply</li><li><strong>Condition</strong> (optional) one or more conditions to satisfy for policy to be applicable, otherwise ignore the policy</li></ul><pre tabindex="0"><code>{
    "Version":"2012-10-17",
    "Statement":[{
        "Sid": "Deny-Barclay-S3-Access",
        "Effect":"Deny",
        "Principal": { "AWS": ["arn:aws:iam:123456789012:barclay"] },
        "Action": [ "s3:GetObject", "s3:PutObject", "s3:List*" ],
        "Resource": ["arn:aws:s3:::mybucket/*"]
    },{
        "Effect": "Allow",
        "Action": "iam:CreateServiceLinkedRole",
        "Resource": "*",
        "Condition": {
            "StringLike": {
                "iam:AWSServiceName": [
                    "rds.amazonaws.com",
                    "rds.application-autoscaling.amazonaws.com"
                ]
            }
        }
    }]
}
</code></pre></li><li><strong>Roles</strong> are associated with trusted entities - AWS services (EC2, Lambda, etc), Another AWS account, Web Identity (Cognito or any OpenID provider), or SAML 2.0 federation (your corporate directory). You attach policy to the role, these entities assume the role to access the AWS resources.</li><li><strong>Least Privilege Principle</strong> should be followed in AWS, don’t give more permission than a user needs.</li><li><strong>Resource Based Policies</strong> are supported by S3, SNS, and SQS</li><li><strong>IAM Permission Boundaries</strong> to set at individual user or role for maximum allowed permissions</li><li><strong>IAM Policy Evaluation Logic</strong> ➔ Explicit Deny ➯ Organization SCPs ➯ Resource-based Policies (optional) ➯ IAM Permission Boundaries ➯ Identity-based Policies</li><li>If you got SSL/TLS certificates from third-party CA, import the certificate into <strong>AWS Certificate Manager (ACM)</strong> or upload it to the <strong>IAM Certificate Store</strong></li></ul><br><h3 id="access-aws-programmatically">Access AWS programmatically</h3><ol><li><strong>AWS Management Console</strong> - Use password + MFA (multi factor authentication)</li><li><strong>AWS CLI or SDK</strong> - Use Access Key ID (~username) and Secret Access Key (~password)<pre tabindex="0"><code>$ aws --version
$ aws configure
AWS Access Key ID [None]:
AES Secret Access Key [None]:
Default region name [None]:
Default output format [None]:
$ aws iam list-users
</code></pre></li><li><strong>AWS CloudShell</strong> - CLI tool from AWS browser console - Require login to AWS</li></ol><br><h3 id="access-aws-for-non-iam-users">Access AWS for Non-IAM users</h3><ul><li>Non-IAM user first authenticate from Identity Federation. Then provide a temporary token (IAM Role attached) generated by calling a AssumeRole API of <strong>STS (Security Token Service)</strong>. Non-IAM user access the AWS resource by assuming IAM Role attached with token.</li><li>You can authenticate and authorize Non-IAM users using following Identity Federation:-<ol><li><strong>SAML 2.0</strong> (old) to integrate Active Directory/ADFS, use AssumeRoleWithSAML STS API</li><li><strong>Custom Identity Broker</strong> used when identity provider is not compatible to SAML 2.0, use AssumeRole or GetFederationToken STS API</li><li><strong>Web Identity Federation</strong> is used to sign in using well-known external identity provider (IdP), such as login with Amazon, Facebook, Google, or any OpenID Connect (OIDC)-compatible IdP. Get the ID token from IdP, use AWS Cognito api to exchange ID token with cognito token, use AssumeRoleWithWebIdentity STS API to get temp security credential to access AWS resources</li><li><strong>AWS Cognito</strong> is recommended identity provider by Amazon</li><li><strong>Amazon Single Sign On</strong> gives single sign-on token to access AWS, no need to call STS API</li></ol></li><li>You can use <strong>AWS Directory Service</strong> to manage Active Directory (AD) in AWS for e.g.<ol><li><strong>AWS Managed Microsoft AD</strong> is managed Microsoft Windows Server AD with trust connection to on-premise Microsoft AD. Best choice when you need all AD features to support AWS applications or Windows workloads. can be used for single sign-on for windows workloads.</li><li><strong>AD Connector</strong> is proxy service to redirect requests to on-premise Microsoft AD. Best choice to use existing on-premise AD with compatible AWS services.</li><li><strong>Simple AD</strong> is standalone AWS managed compatible AD powered by Samba 4 with basic directory features. You cannot connect it to on-premise AD. Best choice for basic directory features.</li><li><strong>Amazon Cognito</strong> is a user directory for sign-up and sign-in to mobile and web application using Cognito User Pools. Nothing to do with Microsoft AD.</li></ol></li></ul><br><h3 id="amazon-cognito">Amazon Cognito</h3><ol><li><strong>Cognito User Pools (CUP)</strong><ul><li>User Pools is a <strong>user directory</strong> for sign-up and sign-in to mobile and web applications.</li><li>User pool is mainly used for <strong>authentication</strong> to access AWS services</li><li>Use to <strong>authenticate</strong> mobile app users through <strong>user pool directory</strong>, or federated through <strong>third-party identity provider (IdP)</strong>. The user pool manages the overhead of handling the tokens that are returned from <strong>social sign-in</strong> through Facebook, Google, Amazon, and Apple, and from <strong>OpenID Connect (OIDC)</strong> and <strong>SAML IdPs</strong>.</li><li>After successful authentication, your web or mobile app will receive user pool <strong>JWT tokens</strong> from Amazon Cognito. JWT token can be used in two ways:-<ol><li>You use JWT tokens to <strong>retrieve temporary AWS credentials</strong> that allow your app to access other AWS services.</li><li>You create group in user pool with IAM role to access API Gateway, then you can use JWT token (for that group) to <strong>access Amazon API Gateway</strong>.</li></ol></li></ul></li><li><strong>Cognito Identity Pools (Federated Identity)</strong><ul><li>Identity pool is mainly used for authorization to access AWS services</li><li>You first authenticate user using <strong>User Pools</strong> and then exchange token with <strong>Identity Pools</strong> which further use <strong>AWS STS</strong> to generate <strong>temporary AWS credentials</strong> to access AWS Resources.</li><li>You can provide temporary access to write to S3 bucket using facebook/google login to your mobile app users.</li><li>Supports <strong>guest users</strong></li></ul></li></ol><br><h3 id="aws-key-management-service-kms">AWS Key Management Service (KMS)</h3><ul><li>AWS managed <strong>centralized key management service</strong> to create, manage and rotate <strong>customer master keys (CMKs)</strong> for encryption at rest.</li><li>You can create customer-managed <strong>Symmetric</strong> (single key for both encrypt and decrypt operations) or <strong>Asymmetric</strong> (public/private key pair for encrypt/decrypt or sign/verify operations) master keys</li><li>You can enable automatic master key rotation once <strong>per year</strong>. Service keeps the older version of master key to decrypt old encrypted data.</li></ul><br><h3 id="aws-cloudhsm">AWS CloudHSM</h3><ul><li>AWS managed <strong>dedicated hardware</strong> security model (HSM) in AWS Cloud</li><li>Enables you to securely generate, store, and manage <strong>your own cryptographic keys</strong></li><li>Integrate with your application using industry-standard APIs, such as PKCS#11, Java Cryptography Extensions (JCE), and Microsoft CryptoNG (CNG) libraries.</li><li><u><strong>Use case:</strong></u> Use <strong>KMS</strong> to create a CMKs in a <strong>custom key store</strong> and store non-extractable key material in AWS CloudHSM to get a <strong>full control on encryption keys</strong></li></ul><br><h3 id="aws-systems-manager">AWS Systems Manager</h3><ul><li><u><strong>Parameter Store</strong></u> is centralized secrets and configuration data management e.g. passwords, database details, and license code<ul><li>Parameter value can be type <strong>String</strong> (plain text), <strong>StringList</strong> (comma separated) or <strong>SecureString</strong> (KMS encrypted data)</li><li><strong>Use case:</strong> Centralized configuration for dev/uat/prod environment to be used by CLI, SDK, and Lambda function</li></ul></li><li><u><strong>Run Command</strong></u> allows you to automate common <strong>administrative tasks</strong> and perform one-time configuration changes on EC2 instances <strong>at scale</strong></li><li><u><strong>Session Manager</strong></u> replaces the need for Bastions to access instances in private subnet</li></ul><br><h3 id="aws-secrets-manager">AWS Secrets Manager</h3><ul><li>Secret Manager is mainly used to store, manage, and rotate secrets (passwords) such as <strong>database credentials, API keys, and OAuth tokens</strong>.</li><li>Secret Manager has <strong>native support to rotate database credentials of RDS databases</strong> - MySQL, PostgreSQL and Amazon Aurora</li><li>For other secrets such as API keys or tokens, you need to use the <strong>lambda for customized rotation function</strong></li></ul><br><h3 id="aws-shield">AWS Shield</h3><ul><li>AWS managed <strong>Distributed Denial of Service (DDoS) protection</strong> service</li><li>Protect against <strong>Layer 3 and 4</strong> (Network and Transport) attacks</li><li><strong>AWS Shield Standard</strong> is automatic and free DDoS protection service for all AWS customers for <u>CloudFront and Route 53 resources</u></li><li><strong>AWS Shield Advanced</strong> is paid service for enhanced DDoS protection for <u>EC2, ELB, CloudFront, and Route 53 resources</u></li></ul><br><h3 id="aws-waf">AWS WAF</h3><ul><li><strong>Web Application Firewall</strong> protects web applications against common web exploits</li><li>Protect against <strong>Layer 7</strong> (HTTP) attack and block common attack patterns, such as <strong>SQL injection</strong> or <strong>Cross-site scripting (XSS)</strong></li><li>You can deploy WAF on <u>CloudFront, Application Load Balancer, API Gateway and AWS AppSync</u></li></ul><br><h3 id="aws-firewall-manager">AWS Firewall Manager</h3><ul><li>Use <strong>AWS Firewall Manager</strong> to centrally configure and manage <u>AWS WAF rules, AWS Shield Advanced, Network Firewall rules, and Route 53 DNS Firewall Rules</u> across accounts and resources in <strong>AWS Organization</strong></li><li><strong>Use case:</strong> Meet Gov regulations to deploy AWS WAF rule to block traffic from embargoed countries across accounts and resources</li></ul><br><h3 id="aws-guardduty">AWS GuardDuty</h3><ul><li>Read <u>VPC Flow Logs, DNS Logs, and CloudTrail events</u>. Apply machine learning algorithms and anomaly detections to discover <strong>threats</strong></li><li>Can protect against <strong>CryptoCurrency</strong> attacks</li></ul><br><h3 id="amazon-inspector">Amazon Inspector</h3><ul><li>Automated Security Assessment service for <strong>EC2 instances</strong> by installing an <strong>agent in the OS</strong> of EC2 instance.</li><li>Inspector comes with <strong>pre-defined rules packages</strong>:-<ol><li><strong>Network Reachability</strong> rules package checks for unintended network accessibility of EC2 instances</li><li><strong>Host Assessment</strong> rules package checks for vulnerabilities and insecure configurations on EC2 instance. Includes Common Vulnerabilities and Exposures (CVE), Center for Internet Security (CIS) Operating System configuration benchmarks, and security best practices.</li></ol></li></ul><br><h3 id="amazon-macie">Amazon Macie</h3><ul><li>Managed service to discover and protect your <strong>sensitive data</strong> in AWS</li><li>Macie identify and alert for sensitive data, such as <strong>Personally Identifiable Information (PII)</strong> in your selected S3 buckets</li></ul><br><h3 id="aws-config">AWS Config</h3><ul><li>Managed service to assess, audit, and evaluate configurations of your AWS resources in multi-region, multi-account</li><li>You are notified via SNS for any configuration change</li><li>Integrated with CloudTrail, provide resource configuration history</li><li><strong>Use case:</strong> Customers need to comply with standards like PCI-DSS (Payment Card Industry Data Security Standard) or HIPAA (U.S. Health Insurance Portability and Accountability Act) can use this service to assess compliance of AWS infra configurations</li></ul><h2 id="compute">Compute</h2><hr><h3 id="ec2-elastic-compute-cloud">EC2 (Elastic Compute Cloud)</h3><ul><li><strong>Infrastructure as a Service (IaaS)</strong> - virtual machine on the cloud</li><li>You must provision <strong>nitro-based EC2</strong> instance to achieve 64000 EBS IOPS. Max 32000 EBS IOPS with Non-Nitro EC2.</li><li>When you restart an EC2 instance, its public IP can change. Use <strong>Elastic IP</strong> to assign a fixed public IPv4 to your EC2 instance. By default, all AWS accounts are limited to five (5) Elastic IP addresses per Region.</li><li>Get EC2 instance metadata such as private &amp; public IP from <code>http://169.254.169.254/latest/meta-data</code> and user-defined data from <code>http://169.254.169.254/latest/user-data</code></li><li>Place all the EC2 instances in same AZ to reduce the data transfer cost</li><li><strong>EC2 Hibernate</strong> saves the contents of <strong>instance memory (RAM)</strong> to the Amazon EBS root volume. When the instance restarts, the RAM contents are reloaded, brings it to last running state, also known as <strong>pre-warm</strong> the instance. You can hibernate an instance only if it’s <strong>enabled for hibernation</strong> and it meets the <strong>hibernation prerequisites</strong></li><li>Use <strong>VM Import/Export</strong> to import virtual machine image and convert to Amazon EC2 AMI to launch EC2 instances</li></ul><h6 id="ec2-instance-types">EC2 Instance Types</h6><p>You can choose <strong>EC2 instance type</strong> based on requirement for e.g. <code>m5.2xlarge</code> has Linux OS, 8 vCPU, 32GB RAM, EBS-Only Storage, Up to 10 Gbps Network bandwidth, Up to 4,750 Mbps IO Operations.</p><table><thead><tr><th style="text-align:left">Instance Class</th><th style="text-align:left">Usage Type</th><th style="text-align:left">Usage Example</th></tr></thead><tbody><tr><td style="text-align:left">T, M</td><td style="text-align:left">General Purpose</td><td style="text-align:left">Web Server, Code Repo, Microservice, Small Database, Virtual Desktop, Dev Environment</td></tr><tr><td style="text-align:left">C</td><td style="text-align:left">Compute Optimized</td><td style="text-align:left">High Performance Computing (HPC), Batch Processing, Gaming Server, Scientific Modelling, CPU-based machine learning</td></tr><tr><td style="text-align:left">R, X, Z</td><td style="text-align:left">Memory Optimized</td><td style="text-align:left">In-memory Cache, High Performance Database, Real-time big data analytics</td></tr><tr><td style="text-align:left">F, G, P</td><td style="text-align:left">Accelerated Computing</td><td style="text-align:left">High GPU, Graphics Intensive Applications, Machine Learning, Speech Recognition</td></tr><tr><td style="text-align:left">D, H, I</td><td style="text-align:left">Storage Optimized</td><td style="text-align:left">EC2 Instance Storage, High I/O Performance, HDFS, MapReduce File Systems, Spark, Hadoop, Redshift, Kafka, Elastic Search</td></tr></tbody></table><h6 id="ec2-launch-types">EC2 Launch Types</h6><ol><li><strong>On-Demand</strong> - pay as you use, pay per hour, costly</li><li><strong>Reserved</strong> - up-front payment and reserve for 1 year or 3 year, two classes:-<ul><li><strong>Standard</strong> unused instanced can be sold in AWS reserved instance marketplace</li><li><strong>Convertible</strong> can be exchanged for another Convertible Reserved Instance with different instance attributes</li></ul></li><li><strong>Scheduled Reserved Instances</strong> - reserve capacity that is scheduled to recur daily, weekly, or monthly, with a specified start time and duration, for a one-year term. After you complete your purchase, the instances are available to launch during the time windows that you specified.</li><li><strong>Spot Instances</strong> - up-to 90% discount, cheapest useful for applications with flexible in timing, can handle interruptions and recover gracefully.<ul><li><strong>Spot blocks</strong> can also be launched with a required duration, which are not interrupted due to changes in the Spot price</li><li><strong>Spot Fleet</strong> is a collection, or fleet, of Spot Instances, and optionally On-Demand Instances, which attempts to launch the number of Spot and On-Demand Instances to meet the specified target capacity</li></ul></li><li><strong>Dedicated Instance</strong> - Your instance runs on dedicated hardware provides physical isolation, single-tenant</li><li><strong>Dedicated Hosts</strong> - Your instances run on a dedicated physical server. More visibility of how instances are placed on server. Let you use existing server-bound software licenses and address corporate compliance and regulatory requirements.</li></ol><p>You have a limit of <strong>20 Reserved instances</strong>, 1152 vCPU On-demand standard instances, and 1440 vCPU spot instances. You can increase the limit by submitting the EC2 limit increase request form.</p><h6 id="ec2-enhanced-networking">EC2 Enhanced Networking</h6><ul><li><strong>Elastic Network Interface (ENI)</strong> is a virtual network card, which you attach to EC2 instance in same AZ. ENI has one primary private IPv4, one or more secondary private IPv4, one Elastic IP per private IPv4, one public IPv4, one or more IPv6, one or more security groups, a MAC address and a source/destination check flag<ul><li>While <strong>primary ENI</strong> cannot be detached from an EC2 instance, A <strong>secondary ENI</strong> with private IPv4 <strong>can be detached and attached</strong> to a standby EC2 instance if primary EC2 becomes unreachable <strong>(failover)</strong></li></ul></li><li><strong>Elastic Network Adapter (ENA)</strong> for C4, D2, and M4 EC2 instances, Upto 100 Gbps network speed.</li><li><strong>Elastic Fabric Adapter (EFA)</strong> is ENA with additional <strong>OS-bypass</strong> functionality, which enables HPC and Machine Learning applications to bypass the operating system kernel and communicate directly with EFA device resulting in very high performance and low latency. for M5, C5, R5, I3, G4, metal EC2 instances.</li><li><strong>Intel 82599 Virtual Function (VF) Interface</strong> for C3, C4, D2, I2, M4, and R3 EC2 instances, Upto 10 Gbps network speed.</li></ul><h6 id="ec2-placement-groups-strategy">EC2 Placement Groups Strategy</h6><p>Placement groups can span across AZs only, <strong>cannot span across regions</strong></p><ol><li><strong>Cluster</strong> - Same AZ, Same Rack, Low latency and High Network, High-Performance Computing (HPC)</li><li><strong>Spread</strong> - Different AZ, Distinct Rack, High Availability, Critical Applications, Limited to 7 instances per AZ per placement group.</li><li><strong>Partition</strong> - Same or Different AZ, Different Rack (or Partition), Distributed Applications like Hadoop, Cassandra, Kafka etc, Upto 7 Partition per AZ</li></ol><h6 id="ami-amazon-machine-image">AMI (Amazon Machine Image)</h6><ul><li>Customized image of an EC2 instance, having built-in OS, softwares, configurations, etc.</li><li>You can create an AMI from EC2 instance and launch a new EC2 instance from AMI.</li><li>AMI are built for a specific region and can be copied across regions</li></ul><h6 id="elb-elastic-load-balancing">ELB (Elastic Load Balancing)</h6><ul><li>AWS load balancer provides a static DNS name provided for e.g. <code>http://myalb-123456789.us-east-1.elb.amazonaws.com</code></li><li>AWS load balancer routes the request to <strong>Target Groups</strong>. Target group can have one or more EC2 instances, IP Addresses or lambda functions.</li><li>Three types of ELB - <u>Classic Load Balancer, Application Load Balancer, and Network Load Balancer</u></li><li><strong>Application Load Balancer (ALB):</strong><ul><li>Routing based on hostname, request path, params, headers, source IP etc.</li><li>Support <strong>Request tracing</strong>, add <code>X-Amzn-Trace-Id</code> header before sending the request to target</li><li>Client IP and port can be found in <code>X-Forwarded-For</code> and <code>X-Forwarded-Porto</code> header</li><li>integrate with WAF with rate-limiting (throttle) rules to prevent from DDoS attacks</li></ul></li><li><strong>Network Load Balancer (NLB):</strong><ul><li>Handle <strong>volatile workloads</strong> and <strong>extreme low-latency</strong></li><li>Provide static IP/Elastic IP for the load balancer per AZ</li><li>allows registering targets by IP address</li><li>Use NLB with Elastic IP in front of ALBs when there is a requirement of whitelisting ALB</li></ul></li><li><strong>Stickiness:</strong> works in CLB and ALB. Stickiness and its duration can be set at Target Group level. Doesn’t work with NLB</li></ul><table><thead><tr><th style="text-align:left">ELB Types</th><th style="text-align:left">Supported Protocol</th></tr></thead><tbody><tr><td style="text-align:left">Application Load Balancer</td><td style="text-align:left">HTTP, HTTPS, WebSocket</td></tr><tr><td style="text-align:left">Network Load Balancer</td><td style="text-align:left">TCP, UDP, TLS</td></tr><tr><td style="text-align:left">Gateway Load Balancer</td><td style="text-align:left">Thirdparty appliances</td></tr><tr><td style="text-align:left">Classic Load Balancer (old)</td><td style="text-align:left">HTTP, HTTPS, TCP</td></tr></tbody></table><h6 id="asg-auto-scaling-group">ASG (Auto Scaling Group)</h6><ul><li>Scale-out (add) or scale-in (remove) EC2 instances based on scaling policy - CPU, Network, Custom metric or Scheduled.</li><li>You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. ASG runs EC2 instances at desired capacity if no policy specified. Minimum and maximum capacity are boundaries within ASG scale-in or scale-out. <code>min &lt;= desired &lt;= max</code></li><li>Instances are created in ASG using <strong>Launch Configuration</strong> (legacy) or <strong>Launch Template</strong> (newer)</li><li>You <strong>cannot change the launch configuration</strong> for an ASG, you must create a new launch configuration and update your ASG with it.</li><li>You can create ASG that launches both Spot and On-Demand Instances or multiple instance types using <strong>launch template</strong>, not possible with launch configuration.</li><li><strong>Dynamic Scaling Policy</strong><ol><li><strong>Target Tracking Scaling</strong> - can have more than one policy for e.g. add or remove capacity to keep the average aggregate CPU utilization of your Auto Scaling group at 40% and request count per target of your ALB target group at 1000 for your ASG. If both policies occurs at same time, use largest capacity for both scale-out and scale-in.</li><li><strong>Simple Scaling</strong> - e.g. CloudWatch alarm CPUUtilization (&gt;80%) - add 2 instances</li><li><strong>Step Scaling</strong> - e.g. CloudWatch alarm CPUUtilization (60%-80%)- add 1, (&gt;80%) - add 3 more, (30%-40%) - remove 1, (&lt;30%) - remove 2 more</li><li><strong>Scheduled Action</strong> - e.g. Increase min capacity to 10 at 5pm on Fridays</li></ol></li><li><strong>Default Termination Policy</strong> - Find AZ with most number of instances, and delete the one with <strong>oldest launch configuration</strong>, in case of tie, the one closest to <strong>next billing hour</strong></li><li><strong>Cooldown period</strong> is the amount of time to wait for previous scaling activity to take effect. Any scaling activity during cooldown period is ignored.</li><li><strong>Health check grace period</strong> is the amount of wait time to check the health status of EC2 instance, which has just came into service to give enough time to warmup.</li><li>You can add <strong>lifecycle-hooks</strong> to ASG to perform custom action during:-<ol><li>scale-out to run script, install softwares and send <code>complete-lifecycle-action</code> command to continue</li><li>scale-in e.g. download logs, take snapshot before termination</li></ol></li></ul><br><h3 id="lambda">Lambda</h3><ul><li>FaaS (Function as a Service), Serverless</li><li>Lambda function <strong>supports many languages</strong> such as Node.js, Python, Java, C#, Golang, Ruby, etc.</li><li>Lambda <strong>limitations</strong>:-<ol><li>execution time can’t exceed 900 seconds or 15 min</li><li>min required memory is 128MB and can go till 10GB with 1-MB increment</li><li><code>/temp</code> directory size to download file can’t exceed 512 MB</li><li>max environment variables size can be 4KB</li><li>compressed <em>.zip</em> and uncompressed code can’t exceed 50MB and 250MB respectively</li></ol></li><li>Lambda function can be <strong>triggered</strong> on DynamoDB database trigger, S3 object events, event scheduled from EventBridge (CloudWatch Events), message received from SNS or SQS, etc.</li><li>Assign IAM Role to lambda function to give access to AWS resource for e.g. create snapshot of EC2, process image and store in S3, etc.</li><li>Lambda can <strong>auto scale in seconds</strong> to handle sudden burst of traffic. EC2 require minutes to auto scale.</li><li>You are charged based on number of requests, execution time and resource (memory) usage. Cheaper than EC2.</li><li>You can use <strong>Lambda@Edge</strong> to run code at CloutFront Edge globally</li><li>You can optionally setup a <strong>dead-letter queue (DLQ)</strong> with SQS or SNS to forward <strong>unprocessed</strong> or failed requests payload</li><li>You can enable and watch the <strong>lambda execution logs</strong> in CloudWatch</li></ul><h2 id="application-integration">Application Integration</h2><hr><h3 id="sqs-amazon-simple-queue-service">SQS (Amazon Simple Queue Service)</h3><ul><li>Fully managed service with following specifications for Standard SQS:-<ol><li>can have unlimited number of messages waiting in queue</li><li>default retention period is 4 days and max 14 days</li><li>can send message upto 256KB in size</li><li>unlimited throughput and low latency (&lt;10ms on publish and receive)</li><li>can have duplicate messages (At least once delivery)</li><li>can have out-of-order messages (best-effort ordering)</li></ol></li><li>Consumer (can be EC2 instance or lambda function) <strong>poll</strong> the messages <strong>in batches</strong> (upto 10 messages) and <strong>delete</strong> them from queue after processing. If don’t delete, they stay in Queue and may process multiple times.</li><li>You should allow Producer and Consumer to send and receive messages from <strong>SQS Queue Access Policy</strong></li><li><strong>Message Visibility Timeout</strong> when a message is polled by a consumer, it becomes <strong>invisible</strong> to other consumers for timeout period.</li><li>You can setup a <strong>Dead-letter queue (DLQ)</strong> which is another SQS to keep the messages which are failed to process by consumers multiple times and exceed the <strong>Maximum receives</strong> threshold in SQS.</li><li>You use <strong>SQS Temporary Queue Client</strong> to implement SQS Request-Response System.</li><li>You can delay message (consumers don’t see them immediately) up to 15 minutes (default 0 seconds). You can do it using <strong>Delivery Delay</strong> configuration at queue level or <strong>DelaySeconds</strong> parameter at message level.</li><li><strong>Long polling</strong> is when the <code>ReceiveMessageWaitTimeSeconds</code> property of a queue is set to a value greater than zero. Long polling reduces the number of empty responses by allowing Amazon SQS to wait until a
message is available before sending a response to a ReceiveMessage request, helps to reduce the cost.</li><li>You can create SQS of type <strong>FIFO</strong> which <strong>guarantee ordering and exactly once processing</strong> with limited throughput upto 300 msg/s without and 3000 msg/s with batching. FIFO queue name must end with suffix <em><strong>.fifo</strong></em>. You can not convert Standard SQS to FIFO SQS.</li><li><u><strong>Use case:</strong></u> Cloudwatch has custom metric on =(SQS queue length/Number of EC2 instances), which alarm ASG to auto scale EC2 instances (SQS consumer) based on number of messages in queue.</li></ul><br><h3 id="sns-amazon-simple-notification-service">SNS (Amazon Simple Notification Service)</h3><ul><li><strong>PubSub</strong> model, where publisher sends the messages on SNS topic and all topic subscribers receive those messages.</li><li>Upto 100,000 topics and Upto 12,500,000 subscription per topic</li><li><strong>Subscribers can be:</strong> Kinesis Data Firehose, SQS, HTTP, HTTPS, Lambda, Email, Email-JSON, SMS Messages, Mobile Notifications.</li><li>You can setup a <strong>Subscription Filter Policy</strong> which is JSON policy to send the filtered messages to specific subscribers.</li><li><strong>Fan out pattern:</strong> SNS topic has multiple SQS subscribers e.g. send all order messages to SNS topic and then send filtered messages based on order status to 3 different application services using SQS.</li></ul><br><h3 id="amazon-mq">Amazon MQ</h3><ul><li>Amazon managed <strong>Apache ActiveMQ</strong></li><li>Migrate an existing message broker using <strong>MQTT</strong> protocol to AWS.</li></ul><h2 id="storage">Storage</h2><hr><h3 id="s3-simple-storage-service">S3 (Simple Storage Service)</h3><ul><li>S3 Bucket is an <strong>object-based</strong> storage, used to manage data as objects</li><li><strong>S3 Object</strong> is having:-<ol><li>Value - data bytes of object (photos, videos, documents, etc.)</li><li>Key - full path of the object in bucket e.g. <code>/movies/comedy/abc.avi</code></li><li>Version ID - version object, if versioning is enabled</li><li>Metadata - additional information</li></ol></li><li>S3 Bucket holds objects. S3 console shows virtual folders based on key.</li><li>S3 is a universal namespace so bucket names must be globally unique (think like having a domain name)<pre tabindex="0"><code>https://&lt;bucket-name&gt;.s3.&lt;aws-region&gt;.amazonaws.com 
or 
https://s3.&lt;aws-region&gt;.amazonaws.com/&lt;bucket-name&gt;
</code></pre></li><li>Unlimited Storage, Unlimited Objects from <strong>0 Bytes</strong> to <strong>5 Terabytes</strong> in size. You should use <strong>multi-part upload</strong> for Object size <strong>&gt; 100MB</strong></li><li>All new buckets are <strong>private</strong> when created by default. You should enable <strong>public access</strong> explicitly.</li><li>Access control can be configured using <strong>Access Control List (ACL)</strong> (deprecated) and <strong>S3 Bucket Policies</strong> (recommended)</li><li><strong>S3 Bucket Policies</strong> are <strong>JSON</strong> based policy for complex access rules at user, account, folder, and object level</li><li>Enable <strong>S3 Versioning</strong> and <strong>MFA delete</strong> features to protect against accidental delete of S3 Object.</li><li>Use <strong>Object Lock</strong> to store object using write-once-read-many (WORM) model to prevent objects from being deleted or overwritten for a fixed amount of time (<strong>Retention period</strong>) or indefinitely (<strong>Legal hold</strong>). Each version of object can have different retention-period.</li><li>You can <strong>host static websites</strong> on S3 bucket consisting of HTML, CSS, <strong>client-side JavaScript</strong>, and images. You need to enable Static website hosting and Public access for S3 to avoid 403 forbidden error. Also you need to add <strong>CORS Policy</strong> to allow cross-origin request.<pre tabindex="0"><code>https://&lt;bucket-name&gt;.s3-website[.-]&lt;aws-region&gt;.amazonaws.com
</code></pre></li><li>Generate a <strong>pre-signed URL</strong> from CLI or SDK (can’t from the web) to provide temporary access to an S3 object to either upload or download object data. You specify expiry (say 5 sec) while generating url:-<pre tabindex="0"><code>aws s3 presign s3://mybucket/myobject --expires-in 300
</code></pre></li><li><strong>S3 Select</strong> or <strong>Glacier Select</strong> can be used to query subset of data from S3 Objects using SQL query. S3 Objects can be CSV, JSON, or Apache Parquet. GZIP &amp; BZIP2 compression is supported with CSV or JSON format with server-side encryption.</li><li>using <code>Range</code> HTTP Header in a GET Request to download the specific range of bytes of S3 object, known as <strong>Byte Range Fetch</strong></li><li>You can create <strong>S3 event notification</strong> to push events e.g. <code>s3:ObjectCreated:*</code> to SNS topic, SQS queue or execute a Lambda function. It is possible that you receive single notification for two writes to a non-versioned object at the same time. Enable versioning to ensure you get all notifications.</li><li>Enable <strong>S3 Cross-Region Replication</strong> for asynchronous replication of object across buckets in another region. You must have <strong>versioning</strong> enabled on both <strong>source</strong> and <strong>destination</strong> side. Only <strong>new S3 Objects</strong> are replicated after you enable them.</li><li>Enable <strong>Server access logging</strong> for logging object-level fields object-size, total time, turn around time, and HTTP referrer. Not available with CloudTrail.</li><li>Use <strong>VPC S3 gateway endpoint</strong> to access S3 bucket within AWS VPC to reduce the overall data transfer cost.</li><li>Enable <strong>S3 Transfer Acceleration</strong> for faster transfer and high throughput to S3 bucket (mainly uploads), Create <strong>CloudFront</strong> distribution with OAI pointing to S3 for faster-cached content delivery (mainly reads)</li><li>Restrict the access of S3 bucket through CloudFront only using <strong>Origin Access Identity (OAI)</strong>. Make sure user can’t use a direct URL to the S3 bucket to access the file.</li></ul><h6 id="s3-storage-class-types">S3 Storage Class Types</h6><ol><li><strong>Standard:</strong> Costly choice for very high availability, high durability and fast retrieval</li><li><strong>Intelligent Tiering:</strong> Uses ML to analyze your Object’s usage and move to the appropriate cost-effective storage class automatically</li><li><strong>Standard-IA:</strong> Cost-effective for infrequent access files which cannot be recreated</li><li><strong>One-Zone IA:</strong> Cost-effective for infrequent access files which can be recreated</li><li><strong>Glacier:</strong> Cheaper choice to Archive Data. You must purchase <strong>Provisioned capacity</strong>, when you require guaranteed <strong>Expedite retrievals</strong>.</li><li><strong>Glacier Deep Archive:</strong> Cheapest choice for Long-term storage of large amount of data for compliance</li></ol><div class="table-scroll"><table><thead><tr><th style="text-align:left">S3 Storage Class</th><th style="text-align:left">Durability</th><th style="text-align:left">Availability</th><th style="text-align:left">AZ</th><th style="text-align:left">Min. Storage</th><th style="text-align:left">Retrieval Time</th><th style="text-align:left">Retrieval fee</th></tr></thead><tbody><tr><td style="text-align:left">S3 Standard (General Purpose)</td><td style="text-align:left">11 9’s</td><td style="text-align:left">99.99%</td><td style="text-align:left">≥3</td><td style="text-align:left">N/A</td><td style="text-align:left">milliseconds</td><td style="text-align:left">N/A</td></tr><tr><td style="text-align:left">S3 Intelligent Tiering</td><td style="text-align:left">11 9’s</td><td style="text-align:left">99.9%</td><td style="text-align:left">≥3</td><td style="text-align:left">30 days</td><td style="text-align:left">milliseconds</td><td style="text-align:left">N/A</td></tr><tr><td style="text-align:left">S3 Standard-IA (Infrequent Access)</td><td style="text-align:left">11 9’s</td><td style="text-align:left">99.9%</td><td style="text-align:left">≥3</td><td style="text-align:left">30 days</td><td style="text-align:left">milliseconds</td><td style="text-align:left">per GB</td></tr><tr><td style="text-align:left">S3 One Zone-IA (Infrequent Access)</td><td style="text-align:left">11 9’s</td><td style="text-align:left"><strong>99.5%</strong></td><td style="text-align:left"><strong>1</strong></td><td style="text-align:left">30 days</td><td style="text-align:left">milliseconds</td><td style="text-align:left">per GB</td></tr><tr><td style="text-align:left">S3 Glacier</td><td style="text-align:left">11 9’s</td><td style="text-align:left">99.99%</td><td style="text-align:left">≥3</td><td style="text-align:left">90 days</td><td style="text-align:left"><strong>Expedite (1-5 mins)</strong><br>Standard (3-5 hrs)<br>Bulk (5-12 hrs)</td><td style="text-align:left">per GB</td></tr><tr><td style="text-align:left">S3 Glacier Deep Archive</td><td style="text-align:left">11 9’s</td><td style="text-align:left">99.99%</td><td style="text-align:left">≥3</td><td style="text-align:left">180 days</td><td style="text-align:left"><strong>Standard (12 hrs)</strong><br>Bulk (48 hrs)</td><td style="text-align:left">per GB</td></tr></tbody></table><div><ul><li>You can upload files in the same bucket with different <strong>Storage Classes</strong> like <em>S3 standard, Standard-IA, One Zone-IA, Glacier</em> etc.</li><li>You can setup <strong>S3 Lifecycle Rules</strong> to transition current (or previous version) objects to cheaper storage classes or delete (expire if versioned) objects after certain days e.g.<ol><li>transition from S3 Standard to <u>S3 Standard-IA or One Zone-IA</u> can only be done after 30 days.</li><li>transition from S3 Standard to <u>S3 Intelligent Tiering, Glacier, or Glacier Deep Archive</u> can be done immediately.</li></ol></li><li>You can also setup lifecycle rule to <strong>abort multipart upload</strong>, if it doesn’t complete within certain days, which auto delete the parts from S3 buckets associated with multipart upload.</li></ul><h6 id="encryption">Encryption</h6><ul><li><strong>Encryption is transit</strong> between client and S3 is achieved via SSL/TLS</li><li>You can add default encryption at bucket level and also override encryption at file level.</li><li><strong>Encryption at rest - Server Side Encryption (SSE)</strong><ol><li><strong>SSE-S3</strong> AWS S3 managed keys, use AES-256 algorithm. Must set header: <code>"x-amz-server-side-encryption":"AES-256"</code></li><li><strong>SSE-KMS</strong> Envelope Encryption using AWS KMS managed keys. Must set header: <code>"x-amz-server-side-encryption":"aws:kms"</code></li><li><strong>SSE-C</strong> Customer provides and manage keys. HTTPS is mandatory.</li></ol></li><li><strong>Encryption at rest - Client Side Encryption</strong> client encrypts and decrypts the data before sending and after receiving data from S3.</li><li>To meet <strong>PCI-DSS or HIPAA</strong> compliance, encrypt S3 using <u>SSE-C and Client Side Encryption</u></li></ul><h6 id="data-consistency">Data Consistency</h6><ul><li>S3 provides <strong>strong read-after-write consistency for PUTs and DELETEs</strong> of objects. PUTs applies to both writes to new objects as well as overwrite existing objects.</li><li><strong>Updates to a single key are atomic.</strong> For example, if you PUT to an existing key from one thread and perform a GET on the same key from a second thread concurrently, you will get either the old data or the new data, but never partial or corrupt data.</li></ul><h6 id="aws-athena">AWS Athena</h6><ul><li>You can use <strong>AWS Athena</strong> (Serverless Query Engine) to perform analytics directly against S3 objects using SQL query and save the analysis report in another S3 bucket.</li><li><strong>Use Case:</strong> one-time SQL query on S3 objects, S3 access log analysis, serverless queries on S3, IoT data analytics in S3, etc.</li></ul><br><h3 id="instance-store">Instance Store</h3><ul><li>Instance Store is temporary <strong>block-based</strong> storage physically attached to an EC2 instance</li><li>Can be attached to an EC2 instance only when the instance is launched and cannot be dynamically resized</li><li>Also known as <strong>Ephemeral Storage</strong></li><li>Deliver very low-latency and high random I/O performance</li><li>Data persists on instance reboot, data <strong>doesn’t persist on stop or termination</strong></li></ul><br><h3 id="ebs-elastic-block-store">EBS (Elastic Block Store)</h3><ul><li>EBS is <strong>block-based</strong> storage, referred as EBS Volume</li><li><strong>EBS Volume</strong> think like a USB stick<ul><li>Can be attached to only one EC2 instance at a time. Can be detached &amp; attached to another EC2 instance in that same AZ only</li><li>Can attach multiple EBS volumes to single EC2 instance. Data persist after detaching from EC2</li></ul></li><li><strong>EBS Snapshot</strong> is a backup of EBS Volume at a point in time. You can not copy EBS volume across AZ but you can create EBS Volume from Snapshot across AZ. EBS Snapshot can copy across AWS Regions.</li><li>Facts about EBS Volume <strong>encryption</strong>:-<ol><li>All data at rest inside the volume is encrypted</li><li>All data in flight between the volume and EC2 instance is encrypted</li><li>All snapshots of encrypted volumes are automatically encrypted</li><li>All volumes created from encrypted snapshots are automatically encrypted</li><li>Volumes created from unencrypted snapshots can be encrypted at the time of creation</li></ol></li><li>EBS supports <strong>dynamic changes in live production</strong> volume e.g. <u>volume type, volume size, and IOPS capacity</u> without service interruption</li><li>There are two types of EBS volumes:-<ol><li><strong>SSD</strong> for small/random IO operations, High IOPS means number of read and write operations per second, Only SSD EBS Volumes can be used as <strong>boot volumes</strong> for EC2</li><li><strong>HDD</strong> for large/sequential IO operations, High Throughput means number of bytes read and write per second</li></ol></li><li>EBS Volumes with two types of RAID configuration:-<ol><li><strong>RAID 0</strong> (increase performance) two 500GB EBS Volumes with 4000 IOPS - creates 1000GB RAID0 Array with 8000 IOPS and 1000Mbps throughput</li><li><strong>RAID 1</strong> (increase fault tolerance) two 500GB EBS Volumes with 4000 IOPS - creates 500GB RAID1 Array with 4000 IOPS and 500Mbps throughput</li></ol></li></ul><table><thead><tr><th style="text-align:left">EBS Volume Types</th><th style="text-align:left">Description</th><th style="text-align:left">Usage</th></tr></thead><tbody><tr><td style="text-align:left">General Purpose SSD (gp2/gp3)</td><td style="text-align:left">Max 16000 IOPS</td><td style="text-align:left">boot volumes, dev environment, virtual desktop</td></tr><tr><td style="text-align:left">Provisioned IOPS SSD (io1/io2)</td><td style="text-align:left">16000 - 64000 IOPS, EBS Multi-Attach</td><td style="text-align:left">critical business application, large SQL and NoSQL database workloads</td></tr><tr><td style="text-align:left">Throughput Optimized HDD (st1)</td><td style="text-align:left">Low-cost, frequently accessed, throughput intensive</td><td style="text-align:left">Big Data, Data warehouses, log processing</td></tr><tr><td style="text-align:left">Cold HDD (sc1)</td><td style="text-align:left">Lowest-cost, infrequently accessed</td><td style="text-align:left">Large data with lowest cost</td></tr></tbody></table><br><h3 id="efs-elastic-file-system">EFS (Elastic File System)</h3><ul><li>EFS is a <strong>POSIX-compliant file-based</strong> storage</li><li>EFS supports <strong>file systems semantics</strong> - strong read-after-write consistency and file locking</li><li><strong>highly scalable</strong> - can automatically scale from gigabytes to petabytes of data without needing to provision storage. With <strong>burst mode</strong>, the throughput increase, as file system grows in size.</li><li><strong>highly available</strong> - stores data redundantly across multiple Availability Zones</li><li>Network File System (NFS) that can be mounted on and <strong>accessed concurrently by thousands of EC2</strong> in multiple AZs without sacrificing performance.</li><li>EFS file systems can be accessed by Amazon EC2 Linux instances, Amazon ECS, Amazon EKS, AWS Fargate, and AWS Lambda functions via a file system interface such as NFS protocol.</li><li><strong>Performance Mode:</strong><ol><li><strong>General Purpose</strong> for most file system for low-latency file operations, good for content-management, web-serving etc.</li><li><strong>Max I/O</strong> is optimized to use with 10s, 100s, and 1000s of EC2 instances with high aggregated throughput and IOPS, <strong>slightly higher latency</strong> for file operations, good for big data analytics, media processing workflow</li></ol></li><li><strong>Use case:</strong> Share files, images, software updates, or computing across all EC2 instances in ECS, EKS cluster</li></ul><br><h3 id="fsx-for-windows">FSx for Windows</h3><ol><li>Windows-based file system supports <strong>SMB</strong> protocol &amp; Windows <strong>NTFS</strong></li><li>supports <strong>Microsoft Active Directory (AD) integration</strong>, ACLs, user quotas</li></ol><br><h3 id="fsx-for-lustre">FSx for Lustre</h3><ul><li>Lustre = Linux + Cluster is a <strong>POSIX-compliant parallel linux file system</strong>, which stores data across multiple network file servers</li><li>High-performance file system for <strong>fast processing of workload</strong> with consistent <strong>sub-millisecond latencies</strong>, up to hundreds of gigabytes per second of throughput, and up to <strong>millions of IOPS</strong>.</li><li>Use it for Machine learning, High-performance computing (HPC), video processing, financial modeling, genome sequencing, and electronic design automation (EDA).</li><li>You can use <strong>FSx for Lustre as hot storage</strong> for your highly accessed files, and <strong>Amazon S3 as cold storage</strong> for rarely accessed files.</li><li><strong>Seamless integration with Amazon S3</strong> - connect your S3 data sets to your FSx for Lustre file system, run your analyses, write results back to S3, and delete your file system</li><li>FSx for Lustre provides two deployment options:-<ol><li><strong>Scratch file systems</strong> - for temporary storage and short-term processing</li><li><strong>Persistent file systems</strong> - for high available &amp; persist storage and long-term processing</li></ol></li></ul><h2 id="database">Database</h2><hr><h3 id="rds-relational-database-service">RDS (Relational Database Service)</h3><ul><li>AWS Managed Service to create PostgreSQL, MySQL, MariaDB, Oracle, Microsoft SQL Server, and Amazon Aurora in the cloud</li><li><strong>Scalability:</strong> Upto 5 Read replicas, replication is asynchronous so reads are eventually consistent.</li><li><strong>Availability</strong> use Multi-AZ Deployment, synchronous replication</li><li>You can create a <strong>read replica</strong> in a <strong>different region</strong> of your running RDS instance. You pay for replication cross Region, but not for cross AZ.</li><li>Automatic <strong>failover by switching the CNAME</strong> from primary to standby database</li><li>Enable <strong>Password and IAM Database Authentication</strong> to authenticate using database password and user credentials through IAM users and roles, works with MySQL and PostgreSQL</li><li>Enable <strong>Enhanced Monitoring</strong> to see percentage of CPU bandwidth and total memory consumed by each database process (OS process thread) in DB instance</li><li>Enable <strong>Automated Backup</strong> for <strong>daily storage volume snapshot</strong> of your DB instance with retention-period from 1 day (default from CLI, SDK) to 7 days (default from console) to 35 days (max). Use <strong>AWS Backup</strong> service for retention-period of 90 days.</li><li>To encrypt an unencrypted RDS DB instance, take a snapshot, copy snapshot and encrypt new snapshot with AWS KMS. Restore the DB instance with the new encrypted snapshot.</li></ul><br><h3 id="amazon-aurora">Amazon Aurora</h3><ul><li>Amazon fully managed relational database compatible with MySQL and PostgreSQL</li><li>Provide 5x throughput of MySQL and 3x throughput of PostgreSQL</li><li><strong>Aurora Global Database</strong> is single database span across multiple AWS regions, enable low-latency global reads and disaster recovery from region-wide outage. Use global database for disaster recovery having RPO of 1 second and RTO of 1 minute.</li><li><strong>Aurora Serverless</strong> capacity type is used for on-demand auto-scaling for intermittent, unpredictable, and sporadic workloads.</li><li>Typically operates as a DB cluster consisting of one or more DB instances and a cluster volume that manages cluster data with each AZ having a copy of volume.<ol><li><strong>Primary DB instance</strong> - Only one primary instance, supports both read and write operation</li><li><strong>Aurora Replica</strong> - Upto 15 replicas spread across different AZ, supports only read operation, automatic failover if primary DB instance fails, high availability</li></ol></li><li>Connections Endpoints<ol><li><strong>Cluster endpoint</strong> - only one cluster endpoint, connects to primary DB instance, only this endpoint can perform write (DDL, DML) operations</li><li><strong>Reader endpoint</strong> - one reader endpoint, provides load-balancing for all read-only connections to read from Aurora replicas</li><li><strong>Custom endpoint</strong> - Upto 5 custom endpoint, read or write from a specified group of DB instances from Cluster, used for specialized workloads to route traffic to high-capacity or low-capacity instances</li><li><strong>Instance endpoint</strong> - connects to specified DB instance directly, generally used to improve connection speed after failover</li></ol></li></ul><br><h3 id="dynamodb">DynamoDB</h3><ul><li>AWS proprietary, Serverless, managed <strong>NoSQL database</strong></li><li>Use to store <strong>JSON documents</strong>, or session data</li><li>Use as distributed serverless cache with <strong>single-digit millisecond</strong> performance</li><li><strong>Planned Capacity</strong> provision WCU &amp; RCU, can enable auto-scaling, good for predictable workloads</li><li><strong>On-demand Capacity</strong> unlimited WCU &amp; RCU, more expensive, good for unpredictable workloads where read &amp; write are less (low throughput)</li><li>Add <strong>DAX (DynamoDB Accelerator) cluster</strong> in front of DynamoDB to cache frequently read values and offload the heavy read on hot keys of DynamoDB, prevent <code>ProvisionedThroughputExceededException</code></li><li>Enable <strong>DynamoDB Streams</strong> to trigger events on database and integrate with lambda function for e.g. send welcome email to user added into the table.</li><li>Use <strong>DynamoDB Global Table</strong> to serve the data globally. You must enable <em>DynamoDB Streams</em> first to create global table.</li><li>You can use <strong>Amazon DMS</strong> (Data Migration Service) to migrate from Mongo, Oracle, MySQL, S3, etc. to DynamoDB</li></ul><br><h3 id="elasticache">ElastiCache</h3><ul><li>AWS Managed Service for <strong>Redis</strong> or <strong>Memcached</strong></li><li>Use as distributed cache with <strong>sub-millisecond</strong> performance</li><li><strong>Elasticache for Redis</strong><ul><li>Offers Multi-AZ with Auto-failover, Cluster mode</li><li>Use password/token to access data using <strong>Redis Auth</strong></li><li>HIPAA Compliant</li></ul></li><li><strong>Elasticache for Memcached</strong><ul><li>Intended for use in speeding up dynamic web applications</li><li>Not HIPAA Compliant</li></ul></li></ul><br><h3 id="redshift">Redshift</h3><ol><li><strong>Columnar Database</strong>, OLAP (online analytical processing)</li><li>supports <strong>Massive Parallel Query Execution (MPP)</strong></li><li>Use for Data Analytics and Data warehousing</li><li>Integrate with <strong>Business Intelligence (BI) tools</strong> like AWS Quicksight or Tableau for analytics</li><li>Use <strong>Redshift Spectrum</strong> to query S3 bucket directly without loading data in Redshift</li></ol><br><h3 id="amazon-kinesis">Amazon Kinesis</h3><p>Amazon Kinesis is a fully managed service for collecting, processing and analyzing <strong>streaming real-time data</strong> in the cloud. Real-time data generally comes from IoT devices, gaming applications, vehicle tracking, clickstream, etc.</p><ol><li><strong>Kinesis Data Streams</strong> capture, process and store data streams.<ul><li>Producer can be <u>Amazon Kinesis Agent, SDK, or Kinesis Producer Library (KPL)</u></li><li>Consumer can be <u>Kinesis Data Analytics, Kinesis Data Firehose, or Kinesis Consumer Library (KCL)</u></li><li>Data Retention period from <strong>24 hours (default)</strong> to 365 days (max).</li><li>Order is maintained at Shard (partition) level.</li></ul></li><li><strong>Kinesis Data Firehose</strong> loads data streams into AWS data stores such as S3, Amazon Redshift and ElastiSearch. Transform data using lambda functions and store failed data in another S3 bucket.</li><li><strong>Kinesis Data Analytics</strong> analyzes data streams with SQL or Apache Flink</li><li><strong>Kinesis Video Streams</strong> capture, process and store video streams</li></ol><br><h3 id="amazon-emr">Amazon EMR</h3><ul><li>EMR = Elastic MapReduce</li><li><strong>Big data</strong> cloud platform for processing vast data using open source tools such as <strong>Hadoop</strong>, Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto.</li><li>EMR can be used to perform data transformation workloads - Extract, transform, load (ETL)</li><li><strong>Use case:</strong> Analyze Clickstream data from S3 using Apache Spark and Hive to deliver more effective ads</li></ul><br><h3 id="neptune">Neptune</h3><ul><li><strong>Graph Database</strong></li><li><strong>Use case:</strong> high relationship data, social networking data, knowledge graphs (Wikipedia)</li></ul><br><h3 id="elasticsearch">ElasticSearch</h3><ul><li>Amazon-managed <strong>Elastic Search</strong> service</li><li>Integration with Kinesis Data Firehose, AWS IoT, and CloudWatch logs</li><li><strong>Use case:</strong> Search, indexing, partial or fuzzy search</li></ul><h2 id="migration">Migration</h2><hr><h3 id="aws-snow-family">AWS Snow Family</h3><ul><li>AWS snow family is used for <strong>on-premises large-scale data migration</strong> to S3 buckets and processing data at low network locations.</li><li>You need to install <strong>AWS OpsHub</strong> software to transfer files from your on-premises machine to snow device.</li><li>You can not migrate directly to Glacier, you should create S3 first with a lifecycle policy to move files to Glacier. You can transfer to Glacier directly using DataSync.</li></ul><table><thead><tr><th style="text-align:left">Family Member</th><th style="text-align:left">Storage</th><th style="text-align:left">RAM</th><th style="text-align:left">Migration Type</th><th style="text-align:left">DataSync</th><th style="text-align:left">Migration Size</th></tr></thead><tbody><tr><td style="text-align:left">Snowcone</td><td style="text-align:left">8TB</td><td style="text-align:left">4GB</td><td style="text-align:left">online &amp; offline</td><td style="text-align:left">yes</td><td style="text-align:left">GBs and TBs</td></tr><tr><td style="text-align:left">Snowball Edge Storage Optimized</td><td style="text-align:left">80TB</td><td style="text-align:left">80GB</td><td style="text-align:left">offline</td><td style="text-align:left">no</td><td style="text-align:left">petabyte scale</td></tr><tr><td style="text-align:left">Snowball Edge Compute Optimized</td><td style="text-align:left">42TB</td><td style="text-align:left">208GB</td><td style="text-align:left">offline</td><td style="text-align:left">no</td><td style="text-align:left">petabyte scale</td></tr><tr><td style="text-align:left">Snowmobile</td><td style="text-align:left">100PB</td><td style="text-align:left">N/A</td><td style="text-align:left">offline</td><td style="text-align:left">no</td><td style="text-align:left">exabyte scale</td></tr></tbody></table><br><h3 id="aws-storage-gateway">AWS Storage Gateway</h3><p>Store gateway is a <strong>hybrid cloud service</strong> to move on-premises data to the cloud and connect on-premises applications with cloud storage.</p><table><thead><tr><th style="text-align:left">Storage Gateway</th><th style="text-align:left">Protocol</th><th style="text-align:left">Backed by</th><th style="text-align:left">Use Case</th></tr></thead><tbody><tr><td style="text-align:left">File Gateway</td><td style="text-align:left">NFS &amp; SMB</td><td style="text-align:left">S3 -&gt; S3-IA, S3 One Zone-IA</td><td style="text-align:left">Store files as object in S3, with a local cache for low-latency access, with user auth using Active Directory</td></tr><tr><td style="text-align:left">FSx File Gateway</td><td style="text-align:left">SMB &amp; NTFS</td><td style="text-align:left">FSx -&gt; S3</td><td style="text-align:left">Windows or Lustre File Server, integration with Microsoft AD</td></tr><tr><td style="text-align:left">Volume Gateway</td><td style="text-align:left">iSCSI</td><td style="text-align:left">S3 -&gt; EBS</td><td style="text-align:left">Block storage in S3 with backups as EBS snapshots.<br>Use <strong>Cached Volume</strong> for low-latency and <strong>Stored Volume</strong> for scheduled backups</td></tr><tr><td style="text-align:left">Tape Gateway</td><td style="text-align:left">iSCSI VTL</td><td style="text-align:left">S3 -&gt;<br>S3 Glacier &amp; Glacier Deep Archive</td><td style="text-align:left">Backup data in S3 and archive in Glacier using tape-based process</td></tr></tbody></table><br><h3 id="aws-datasync">AWS DataSync</h3><ol><li>AWS DataSync is used for <strong>Data Migration</strong> at a large scale from <strong>On-premises storage</strong> systems (using NFS and SMB storage protocol) to <strong>AWS storage</strong> (like S3, EFS, or FSx for Windows, AWS Snowcone) over the <strong>internet</strong></li><li>AWS DataSync is used to archive on-premises <strong>cold data</strong> directly to <strong>S3 Glacier or S3 Glacier Deep Archive</strong></li><li>AWS DataSync can migrate data directly to <strong>any S3 storage class</strong></li><li>Use DataSync with <strong>Direct Connect</strong> to migrate data over <strong>secure private network</strong> to AWS service associated with <strong>VPC endpoint</strong>.</li></ol><br><h3 id="aws-backup">AWS Backup</h3><ol><li>AWS Backup to centrally manage and automate the backup process for <u>EC2 instances, EBS Volumes, EFS, RDS databases, DynamoDB tables, FSx for Lustre, FSx for Window server, and Storage Gateway volumes</u></li><li><strong>Use case:</strong> Automate backup of RDS with 90 days retention policy. (Automate backup using RDS directly has max 35 days retention period)</li></ol><br><h3 id="database-migration-service-dms">Database Migration Service (DMS)</h3><ul><li>DMS helps you to migrate database to AWS with source remaining fully operational during the migration, minimizing the downtime</li><li>You need to select EC2 instance to run <strong>DMS</strong> in order to migrate (and replicate) database from source =&gt; target e.g. On-premise =&gt; AWS, AWS =&gt; AWS, or AWS =&gt; On-premise</li><li>DMS supports both <strong>homogenous</strong> migrations such as <u>On-premise PostgreSQL =&gt; AWS RDS PostgreSQL</u> and <strong>heterogenous</strong> migrations such as <u>SQL Server or Oracle =&gt; MySQL, PostgreSQL, Aurora</u>, or <u>Teradata or Oracle =&gt; Amazon Redshift</u></li><li>You need to run <strong>AWS SCT</strong> (Schema Conversion Tool) at source for <strong>heterogenous</strong> migrations</li></ul><br><h3 id="aws-application-migration-service-mgn">AWS Application Migration Service (MGN)</h3><ul><li>Migrate virtual machines from <u>VMware vSphere, Microsoft Hyper-V or Microsoft Azure</u> to AWS</li><li><strong>AWS Application Migration Service</strong> (new) utilizes continuous, block-level replication and enables cutover windows measured in minutes</li><li><strong>AWS Server Migration Service</strong> (legacy) utilizes incremental, snapshot-based replication and enables cutover windows measured in hours.</li></ul><h2 id="networking--content-delivery">Networking &amp; Content Delivery</h2><hr><h3 id="amazon-vpc">Amazon VPC</h3><ol><li><strong>CIDR block</strong> — Classless Inter-Domain Routing. An internet protocol address allocation and route aggregation methodology. CIDR block has two components - Base IP (WW.XX.YY.ZZ) and Subnet Mask (/0 to /32) for e.g.<ul><li><strong>192.168.0.0/32</strong> means 2<sup>32-32</sup>= 1 single IP</li><li><strong>192.168.0.0/24</strong> means 2<sup>32-24</sup>= 256 IPs ranging from <em>192.168.0.0 to 192.168.0.255</em> (last number can change)</li><li><strong>192.168.0.0/16</strong> means 2<sup>32-16</sup>= 65,536 IPs ranging from <em>192.168.0.0 to 192.168.255.255</em> (last 2 numbers can change)</li><li><strong>192.168.0.0/8</strong> means 2<sup>32-8</sup>= 16,777,216 IPs ranging from <em>192.0.0.0 to 192.255.255.255</em> (last 3 numbers can change)</li><li><strong>0.0 0.0.0.0/0</strong> means 2<sup>32-0</sup>= All IPs ranging from <em>0.0.0.0 to 255.255.255.255</em> (all 4 numbers can change)</li></ul></li><li><strong>VPC (Virtual Private Cloud)</strong><ul><li>A virtual network dedicated to your AWS account.</li><li>VPCs are <strong>region specific</strong> they do not span across regions</li><li>Every region comes with default VPC. You can create <strong>upto 5 VPC</strong> per region.</li><li>You can assign <strong>Max 5 IPv4 CIDR blocks</strong> per VPC with <strong>min</strong> block size <strong>/28</strong> = 16 IPs and <strong>max</strong> size <strong>/16</strong> = 65,536 IPs. You can assign Secondary IP CIDR range later if primary CIDR IPs are exhausted.</li><li>Only private IP ranges are allowed in IPv4 CIDR block - 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16.</li><li>You VPC CIDR block should not overlap with other VPC networks within your AWS account.</li><li>Enable <strong>DNS resolution</strong> and <strong>DNS hostnames</strong> at VPC, EC2 instances created in that VPC will be assigned a domain name address</li></ul></li><li><strong>VPC Peering</strong><ul><li>VPC peering connect two VPC over a <strong>direct network route</strong> using <strong>private IP addresses</strong></li><li>Instances on peered VPCs <strong>behave</strong> just like they are on the <strong>same network</strong></li><li>Must have <strong>no overlapping CIDR Blocks</strong></li><li>VPC peering connection are <strong>not transitive</strong> i.e. VPC-A peering VPC-B and VPC-B peering to VPC-C doesn’t mean VPC-A peering VPC-C</li><li>Route tables <strong>must be updated</strong> in both VPC that are peered so that instances can communicate</li><li>Can connect one VPC to another in <strong>same</strong> or <strong>different region</strong>. VPC peering in the different region called <strong>VPC inter-region peering</strong></li><li>Can connect one VPC to another in <strong>same</strong> or <strong>different AWS account</strong></li></ul></li><li><strong>Subnet</strong><ul><li>A range of IP addresses in your VPC</li><li>Each subnet is tied to one Availability Zone, one Route Table, and one Network ACL</li><li>You assign one CIDR block per Subnet within CIDR range of your VPC. Should not overlap with other Subnet’s CIDR in your VPC.</li><li>AWS <strong>reserve 5 IP address</strong> (first 4 and last 1) from CIDR block in each Subnet. For e.g. If you need 29 IP addresses to use, your should choose CIDR /26 = 64 IP and not /27 = 32 IP, since 5 IPs are reserved and can not use.</li><li>Enable <strong>Auto assign public IPv4</strong> address in public subnets, EC2 instances created in public subnets will be assigned a public IPv4 address</li><li>If you have 3 AZ in a region then you create a total of 6 subnets - 3 private subnets (1 in each AZ) and 3 public subnets (1 in each AZ) for multi-tier and highly-available architecture. API gateway and ALB reside in the public subnet, EC2 instances, Lambda, Database resides in private subnet.</li></ul></li><li><strong>Route Table</strong><ul><li>A set of rules, called routes, are used to determine where <strong>network traffic is directed</strong>.</li><li>Each <strong>subnet</strong> in your VPC <strong>must be associated</strong> with a route table.</li><li>A subnet can only be associated <strong>with one route table at a time</strong></li><li>You can associate <strong>multiple subnets with the same route table</strong> For e.g. you create 4 subnets in your VPC where 2 subnets associated with one route table with no internet access rules known as <strong>private subnets</strong> and another 2 subnets are associated with another route table with internet access rules known as <strong>public subnets</strong></li><li>Each Route table route has <strong>Destination</strong> like IPs and <strong>Target</strong> like local, IG, NAT, VPC endpoint etc.</li><li><strong>public subnet</strong> is a subnet that’s associated with a route table having rules to connect to internet using Internet Gateway.</li><li><strong>private subnet</strong> is a subnet that’s associated with a route table having no rules to connect to internet using Internet Gateway. When our Subnets connected to the Private Route Table need access to the internet, we set up a NAT Gateway in the public Subnet. We then add a rule to our Private Route Table saying that all traffic looking to go to the internet should point to the NAT Gateway.</li></ul></li><li><strong>Internet Gateway</strong><ul><li>Internet Gateway allows AWS instances <strong>public subnet access to the internet and accessible from the internet</strong></li><li>Each Internet Gateway is associated with one VPC only, and each VPC has one Internet Gateway only (one-to-one mapping)</li></ul></li><li><strong>NAT Gateway</strong><ul><li>NAT Gateway allows AWS instances in <strong>private subnet access to the internet but not accessible from the internet</strong></li><li><strong>NAT Gateway</strong> (latest) is a managed service that launches redundant instances within the selected AZ (can survive failure of EC2 instance)</li><li><strong>NAT Instances</strong> (legacy) are individual EC2 instances. Community AMIs exist to launch NAT Instances. Works same as NAT Gateway.</li><li>You can only have 1 NAT Gateway inside 1 AZ (cannot span AZ).</li><li>You should create a NAT Gateway in each AZ for <strong>high availability</strong> so that if a NAT Gateway goes down in one AZ, instances in other AZs are still able to access the internet.</li><li>NAT Gateway resides in public subnet. You must <strong>allocate Elastic IP</strong> to NAT Gateway. You must add NAT Gateway in <strong>private subnet route table</strong> with Destination <code>0.0.0.0/0</code> and Target <code>nat-gateway-id</code></li><li>NAT Gateways are <strong>automatically assigned a public IP address</strong></li><li>NAT Gateway/Instances <strong>works with IPv4</strong></li><li>NAT Gateway <strong>cannot be shared across VPC</strong></li><li>NAT Gateway <strong>cannot be used as Bastions</strong> whereas Nat Instance can</li></ul></li><li><strong>Bastion Host</strong><ul><li>Bastian Host is an individual <strong>small EC2 instance in public subnet</strong>. Community AMIs exist to launch Bastion Host.</li><li>Bastian Host are used to access AWS instances in <strong>private subnet with private IPv4 address via SSH at port 22</strong></li></ul></li><li><strong>Egress Only Internet Gateway</strong><ul><li>Works same as NAT Gateway, but <strong>for IPv6</strong></li><li>Egress Only means - outgoing traffic only</li><li>IPv6 are public by default. Egress Only Internet Gateway allows IPv6 instances in private subnet access to the internet but accessible from internet</li></ul></li><li><strong>Network ACL</strong><ul><li>Network Access Control List is commonly <strong>known as NACL</strong></li><li>Optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.</li><li>VPCs comes with a modifiable <strong>default NACL</strong>. By default, it <strong>allows all inbound and outbound</strong> traffic.</li><li>You can create <strong>custom NACL</strong>. By default, each custom network ACL <strong>denies all inbound and outbound</strong> traffic until you add rules.</li><li>Each subnet within a VPC must be associated with only 1 NACL<ul><li>If you don’t specify, auto associate with default NACL.</li><li>If you associate with a new NACL, auto-remove previous association</li><li>Apply to all instances in associated subnet</li></ul></li><li>Support both <strong>Allow</strong> and <strong>Deny</strong> rules</li><li><strong>Stateless</strong> means explicit rules for inbound and outbound traffic. return traffic must be explicitly allowed by rules</li><li>Evaluate rules in <strong>number order</strong>, starting with lowest numbered rule. NACL rules have number(1 to 32766) and higher precedence to lowest number for e.g. <code>#100 ALLOW &lt;IP&gt;</code> and <code>#200 DENY &lt;IP&gt;</code> means IP is allowed</li><li>Each network ACL also includes a rule with <strong>rule number as asterisk</strong> <code>*</code>. If any of the numbered rule doesn’t match, it’s denies the traffic. You can’t modify or remove this rule.</li><li>Recommended creating numbered rules in increments (for example, increments of 10 or 100) so that you can insert new rules where you need to later on.</li><li>You can <strong>block a single IP address</strong> using NACL, which you can’t do using Security Group</li></ul></li><li><strong>Security Group</strong><ul><li>Control inbound and outbound traffic <strong>at EC2 instance level</strong></li><li>Support <strong>Allow</strong> rules only. All traffic is <strong>deny</strong> by default unless a rule specifically allows it.</li><li><strong>Stateful</strong> means return traffic is automatically allowed, regardless of any rules</li><li>When you first create a security group, It has no inbound rule means <strong>denies all incoming</strong> traffic and one outbound rule that <strong>allows all outgoing</strong> traffic.</li><li>You can specify a source in the security group rule to be an <strong>IP range, A specific IP (/32), or another security group</strong></li><li>One security group can be associated with <strong>multiple instances across multiple subnets</strong></li><li>One EC2 instance can be associated with <strong>multiple Security Groups</strong> and rules are <strong>permissive</strong> (instead of restrictive). Meaning if you have one security group which has no Allow and you add an allow in another then it will Allow</li><li>Evaluate all rules before deciding whether to allow traffic</li></ul></li><li><strong>Transit gateway</strong> is used to create transitive VPC peer connections between thousands of VPCs<ul><li>hub-and-spoke (star) connection</li><li>Support <strong>IP Multicast</strong> (not supported by any other AWS service)</li><li>Use as gateway at Amazon side in VPN connection, not at customer side</li><li>Can be attached to - one or more VPCs, AWS Direct Connect gateway, VPN Connection, peering connection to another Transit gateway</li></ul></li><li><strong>VPC Flow Logs</strong><ul><li>Allows you to capture <strong>IP traffic information</strong> in-and-out of Network Interfaces within your VPC</li><li>You can turn on Flow Logs at <u>VPC, Subnet or Network Interface level</u></li><li>VPC Flow logs can be delivered to <strong>S3</strong> or <strong>CloudWatch logs</strong>. Query VPC flow logs using Athena on S3 or CloudWatch logs insight</li><li>VPC Flow logs have - Log Version <code>version</code>, AWS Account Id <code>account-id</code>, Network Interface Id <code>interface-id</code>, Source IP address and port <code>srcaddr</code> &amp; <code>srcport</code>, destination IP address and port <code>dstaddr</code> &amp; <code>dstport</code></li><li>VPC Flow logs contain source and destination <strong>IP addresses</strong> (not hostnames)</li></ul></li><li><strong>IPv6</strong> are all public addresses, all instances with IPv6 are publicly accessible. for private ranges, we still use IPv4. You can not disable IPv4. If you enable IPv6 for VPC and subnets, then your EC2 instance would get private IPv4 and public IPv6</li><li><strong>Cost nothing:</strong> VPCs, Route Tables, NACLs, Internet Gateway, Security Groups, Subnets, VPC Peering</li><li><strong>Cost money:</strong> NAT Gateway, VPC Endpoints, VPN Gateway, Customer Gateway</li></ol><br><h3 id="vpc-endpoints">VPC endpoints</h3><ul><li>VPC endpoints allow <strong>your VPC to connect to other AWS services privately</strong> within the AWS network</li><li>Traffic between your VPC and other services <strong>never leaves the AWS network</strong></li><li>Eliminates the need for an Internet Gateway and NAT Gateway for instances in public and private subnets to access the other AWS services through public internet.</li><li>There are two types of VPC endpoints:-<ol><li><strong>Interface endpoint</strong> are Elastic Network Interfaces (ENI) with a private IP address. They serve as an entry point for traffic going to most of the AWS services. Interface endpoints are provided by <strong>AWS PrivateLink</strong> and have an hourly fee and per GB usage cost.</li><li><strong>Gateway endpoint</strong> is a gateway that is a target <strong>for a specific route</strong> in your <strong>route table</strong>, use to destined for a supported AWS service. Currently supports only <strong>Amazon S3 and DynamoDB</strong>. Gateway endpoints <strong>are free</strong></li></ol></li><li>If EC2 instance wants to access S3 bucket or DynamoDB in <strong>different region privately</strong> within AWS network then we first need <strong>VPC inter-region peering</strong> to connect VPC in both regions and then use VPC gateway endpoint for S3 or DynamoDB.</li><li><strong>AWS PrivateLink</strong> is VPC interface endpoint service to expose a particular service to 1000s of VPCs cross-accounts</li><li><strong>AWS ClassicLink</strong> (deprecated) to connect EC2-classic instances privately to your VPC</li></ul><br><h3 id="aws-vpn">AWS VPN</h3><ol><li><strong>AWS Site-to-Site VPN</strong> connection is created to communicate between your remote network and Amazon VPC <strong>over the internet</strong></li><li><strong>VPN connection:</strong> A secure connection between your on-premises equipment and your Amazon VPCs.</li><li><strong>VPN tunnel:</strong> An encrypted link where data can pass from the customer network to or from AWS. Each VPN connection includes two VPN tunnels which you can simultaneously use for high availability.</li><li><strong>Customer gateway:</strong> An AWS resource that provides information to AWS about your customer gateway device.</li><li><strong>Customer gateway device:</strong> A physical device or software application on the customer side of the Site-to-Site VPN connection.</li><li><strong>Virtual private gateway:</strong> The VPN concentrator on the Amazon side of the Site-to-Site VPN connection. You use a virtual private gateway or a transit gateway as the gateway for the Amazon side of the Site-to-Site VPN connection.</li><li><strong>Transit gateway:</strong> A transit hub that can be used to interconnect your VPCs and on-premises networks. You use a transit gateway or virtual private gateway as the gateway for the Amazon side of the Site-to-Site VPN connection.</li></ol><br><h3 id="aws-direct-connect">AWS Direct Connect</h3><ul><li>Establish a <strong>dedicated private connection</strong> from On-premises locations to the AWS VPC network.</li><li>Can access public resources (S3) and private (EC2) on the same connection</li><li>Provide 1GB to 100GB/s network bandwidth for fast transfer of data from on-premises to Cloud</li><li>Not an immediate solution, because it takes a few days to establish a new direction connection</li></ul><br><table><thead><tr><th style="text-align:left">AWS VPN</th><th style="text-align:left">AWS Direct Connect</th></tr></thead><tbody><tr><td style="text-align:left">Over the internet connection</td><td style="text-align:left">Over the dedicated private connection</td></tr><tr><td style="text-align:left">Configured in minutes</td><td style="text-align:left">Configured in days</td></tr><tr><td style="text-align:left">low to modest bandwidth</td><td style="text-align:left">high bandwidth 1 to 100 GB/s</td></tr></tbody></table><br><h3 id="amazon-api-gateway">Amazon API Gateway</h3><ul><li>Serverless, Create and Manage APIs that act as a front door for back-end systems running on EC2, AWS Lambda, etc.</li><li>API Gateway Types - HTTP, WebSocket, and REST</li><li>Allows you to track and control the usage of API. Set <strong>throttle limit</strong> (default 10,000 req/s) to prevent being overwhelmed by too many requests and returns 429 <code>Too Many Request</code> error response. It uses the bucket-token algorithm where the <strong>burst size</strong> is the max bucket size. For a throttle limit of 10000 req/s and a burst of 5000 requests, if 8000 requests are coming in the first millisecond, then 5000 are served immediately and throttle the rest 3000 in the one-second period.</li><li>Caching can be enabled to cache your API response to reduce the number of API calls and improve latency</li><li><strong>API Gateway Authentication</strong><ol><li><strong>IAM</strong> Policy is used for authentication and authorization of AWS users and leverage <strong>Sig v4</strong> to pass IAM credential in the request header</li><li><strong>Lambda Authorizer</strong> (formerly Custom Authorizer) use lambda for OAuth, SAML or any other 3rd party authentication</li><li><strong>Cognito User Pools</strong> only provide authentication. Manage your own user pool (can be backed by Facebook, Google, etc.)</li></ol></li></ul><br><h3 id="amazon-cloudfront">Amazon CloudFront</h3><ul><li>It’s a <strong>Content Delivery Network (CDN)</strong> that uses <strong>AWS edge locations</strong> to cache and deliver <strong>cached content</strong> (such as images and videos)</li><li>CloudFront can cache data from <strong>Origin</strong> for e.g.<ol><li>S3 bucket using OAI (Origin Access Identity) and S3 bucket policy</li><li>EC2 or ALB if they are public and security group allows</li></ol></li><li><strong>Origin Access Identity (OAI)</strong> can be used to restrict the content from S3 origin to be accessible from CloudFront only</li><li>supports <strong>Geo restriction (Geo-Blocking)</strong> to whitelist or blacklist countries that can access the content</li><li>supports <strong>Web download</strong> distribution (static, dynamic web content, video streaming) and <strong>RTMP Streaming</strong> distribution (media files from Adobe media server using RTMP protocol)</li><li>You can generate a <strong>Signed URL</strong> (for a single file and RTMP streaming) or <strong>Signed Cookie</strong> (for multiple files) to share content with premium users</li><li>integrates with AWS WAF, a web application firewall to protect from layer 7 attacks</li><li>Objects are removed from the cache upon <strong>expiry (TTL)</strong>, by default 24 hours.</li><li><strong>Invalidate the Object</strong> explicitly for web distribution only with the cost associated, which removes the object from CloudFront cache. Otherwise, you can change the object name, and <strong>versioning</strong> to serve new content.</li></ul><br><h3 id="amazon-route-53">Amazon Route 53</h3><ul><li>AWS Managed Service to create DNS Records (Domain Name System)</li><li>Browser cache the resolved IP from DNS for TTL (time to live)</li><li>Expose public IP of EC2 instances or load balancer</li><li><strong>Domain Registrar</strong> If you want to use Route 53 for domains purchased from 3rd party websites like GoDaddy.<ul><li>AWS - You need to create a <strong>Hosted Zone</strong> in Route 53</li><li>GoDaddy - update the 3rd party registrar NS (name server) records to use Route 53.</li></ul></li><li><strong>Private Hosted Zone</strong> is used to create an internal (intranet) domain name to be used within Amazon VPC. You can then add some DNS records and routing policies for that internal domain. That internal domain is accessible from EC2 instances or any other resource within VPC.</li></ul><h6 id="dns-record-type">DNS Record: Type</h6><ol><li><strong>CNAME</strong> points hostname to any other hostname. Only works with <strong>subdomains</strong> e.g. <code>something.mydomain.com</code></li><li><strong>A or AAAA (Alias)</strong> points hostname to an AWS Resource like ALB, API Gateway, CloudFront, S3 Bucket, Global Accelerator, Elastic Beanstalk, VPC interface endpoint etc. Works with both root-domain and subdomains e.g. <code>mydomain.com</code>. <strong>AAAA</strong> is used for IPv6 addresses.</li></ol><h6 id="dns-record-routing-policy">DNS Record: Routing Policy</h6><ol><li><strong>Simple</strong> to route traffic to specific IP using a single DNS record. Also allows you to return multiple IPs after resolving DNS.</li><li><strong>Weighted</strong> to route traffic to different IPs based on weights (between 0 to 255) e.g. create 3 DNS records for weights 70, 20, and 10.</li><li><strong>Latency</strong> to route traffic to different IPs based on AWS regions nearest to the client for low-latency e.g. create 3 DNS records with region us-east-1, eu-west-2, and ap-east-1</li><li><strong>Failover</strong> to route traffic from Primary to Secondary in case of failover e.g. create 2 DNS records for primary and secondary IP. It is mandatory to create health check for both IP and associate to record.</li><li><strong>Geolocation</strong> to route traffic to specific IP based on user geolocation (select Continent or Country). Should also create default (select Default location) policy in case there’s no match on location.</li><li><strong>Geoproximity</strong> to route traffic to specific IP based on user geolocation and <strong>bias</strong> value. Positive bias (1 to 99) for more traffic and negative bias (-1 to -99) for less traffic. You can <strong>control the traffic</strong> from specific geolocation using bias value.</li><li><strong>Multivalue Answer</strong> to return up to 8 healthy IPs after resolving DNS e.g. create 3 DNS records with an associated health check. Acts as client-side Load Balancer, expect a downtime of TTL, if an EC2 becomes unhealthy.</li></ol><h6 id="dns-failover">DNS Failover</h6><ol><li><strong>active-active failover</strong> when you want all resources to be available the majority of the time. All records have the same name, same type, and same routing policy such as <strong>weighted or latency</strong></li><li><strong>active-passive failover</strong> when you have active primary resources and standby secondary resources. You create two records - primary &amp; secondary with <strong>failover</strong> routing policy</li></ol><br><h3 id="aws-global-accelerator">AWS Global Accelerator</h3><ul><li>Global Service</li><li>Global Accelerator <strong>improves the performance</strong> of your application <strong>globally</strong> by <strong>lowering latency and jitter</strong>, and increasing throughput as compared to the public internet.</li><li>Use <u>Edge locations and AWS internal global network</u> to find an <strong>optimal pathway</strong> to route the traffic.</li><li>First, you create a global accelerator, which provisions <strong>two anycast static IP addresses</strong>.</li><li>Then you register <strong>one or more endpoints</strong> with Global Accelerator. <strong>Each endpoint can have one or more AWS resources</strong> such as NLB, ALB, EC2, S3 Bucket or Elastic IP.</li><li>You <strong>can set the weight</strong> to choose how much traffic is routed to each endpoint.</li><li>Within the endpoint, global accelerator <strong>monitors health checks</strong> of all AWS resources to send traffic to healthy resources only</li></ul><h2 id="management--governance">Management &amp; Governance</h2><hr><h3 id="amazon-cloudwatch">Amazon CloudWatch</h3><ul><li>CloudWatch is used to <strong>collect &amp; track metrics, collect &amp; monitor log files, and set alarms</strong> of AWS resources like EC2, ALB, S3, Lambda, DynamoDB, RDS etc.</li><li>By default, CloudWatch will aggregate and store the metrics at Standard 1-minute resolution. You can set max high-resolution at 1 second.</li><li>CloudWatch dashboard can include graphs from <strong>different AWS accounts and regions</strong></li><li>CloudWatch has the following EC2 instance metrics - <u>CPU Utilization %, Network Utilization, and Disk Read Write</u>. You need to set up a custom metric for Memory Utilization, Disk Space Utilization, SwapUtilization etc.</li><li>You need to install <strong>CloudWatch Logs Agent</strong> on EC2 to collect custom metrics and logs on CloudWatch</li><li>You can terminate or recover EC2 instances based on <strong>CloudWatch Alarm</strong></li><li>You can schedule a Cron job using <strong>CloudWatch Events</strong></li><li>Any AWS service should have access to <code>log:CreateLogGroup</code>, <code>log:CreateLogStream</code>, and <code>log:PutLogEvents</code> actions to write logs to CloudWatch</li></ul><br><h3 id="aws-cloudtrail">AWS CloudTrail</h3><ul><li>CloudTrail provides <strong>audit and event history</strong> of all the actions taken by any user, AWS service, CLI, or SDK across AWS infrastructure.</li><li>CloudTrail is enabled (applied) by default for all regions</li><li>CloudTrail logs can be sent to CloudWatch logs or S3 bucket</li><li><strong>Use case:</strong> check in the CloudTrail if any resource is deleted from AWS without anyone’s knowledge.</li></ul><br><h3 id="aws-cloudformation">AWS CloudFormation</h3><ul><li>Infrastructure as Code (IaC). Enable modeling, provisioning, and versioning of your entire infrastructure in a text (.YAML) file</li><li>Create, update, or delete your <strong>stack of resources</strong> using CloudFormation <strong>template as a JSON or YAML file</strong></li><li>CloudFormation template has a following components:-<ol><li><strong>Resources:</strong> AWS resources declared in the template (mandatory)</li><li><strong>Parameters:</strong> input values to be passed in the template at stack creation time</li><li><strong>Mappings:</strong> Static variables in the template</li><li><strong>Outputs:</strong> Output which you want to see once the stack is created e.g. return ElasticIP address after attaching to VPC, return DNS of ELB after stack creation.</li><li><strong>Conditionals:</strong> List of conditions to perform resource creation</li><li>Metadata</li><li>Template helpers: References and Functions</li></ol></li><li>Allows <strong>DependsOn</strong> attribute to specify that the creation of a specific resource follows another</li><li>Allows <strong>DeletionPolicy</strong> attribute to be defined <strong>for resources</strong> in the template<ol><li><strong>retain</strong> to preserve resources like S3 even after stack deletion</li><li><strong>snapshot</strong> to backup resources like RDS after stack deletion</li></ol></li><li>Supports <strong>Bootstrap scripts</strong> to install packages, files and services on the EC2 instances by simply describing them in the template</li><li><strong>automatic rollback on error</strong> feature is enabled, by default, which will cause all the AWS resources that CF created successfully for a stack up to the point where an error occurred to be deleted</li><li>AWS CloudFormation <strong>StackSets</strong> allow you to create, update or delete CloudFormation <strong>stacks across multiple accounts, regions, OUs in AWS organization</strong> with a single operation.</li><li>Using CloudFormation itself is free, underlying AWS resources are charged</li><li><strong>Use case:</strong> Use to set up the same infrastructure in different environments e.g. SIT, UAT and PROD. Use to create DEV resources every day in working hours and delete them later to lower the cost</li></ul><br><h3 id="aws-elastic-beanstalk">AWS Elastic Beanstalk</h3><ul><li>Platform as a Service (PaaS)</li><li>Makes it easier for developers to quickly deploy and manage applications without thinking about underlying resources</li><li>Automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling and application health monitoring</li><li>You can launch an <strong>application</strong> with the following pre-configured platforms:-<ul><li>Apache Tomcat for Java applications,</li><li>Apache HTTP Server for PHP and Python applications</li><li>Nginx or Apache HTTP Server for Node.js applications</li><li>Passenger or Puma for Ruby applications</li><li>Microsoft IIS 7.5 for .NET applications</li><li>Single and Multi Container <strong>Docker</strong></li></ul></li><li>You can also launch an <strong>environment</strong> with the following environment tier:-<ul><li>An application that serves HTTP requests runs in a <strong>web server environment tier</strong>.</li><li>A backend environment that pulls tasks from an Amazon Simple Queue Service (Amazon SQS) queue runs in a <strong>worker environment tier</strong>.</li></ul></li><li>It costs nothing to use Elastic Beanstalk, only the resources it provisions e.g. EC2, ASG, ELB, and RDS etc.</li><li>supports custom AMI to be used</li><li>supports <strong>multiple running environments</strong> for development, staging and production, etc.</li><li>supports <strong>versioning</strong> and stores and tracks application versions over time allowing easy rollback to prior version</li></ul><br><h3 id="aws-parallelcluster">AWS ParallelCluster</h3><ul><li>Deploy and manage High-Performance Computing (HPC) clusters on AWS using a simple text file</li><li>You have full control of the underlying resources.</li><li>AWS ParallelCluster is free, and you pay only for the AWS resources needed to run your applications.</li><li>You can configure HPC cluster with Elastic Fabric Adapter (EFA) to get OS-bypass capabilities for low-latency network communication</li></ul><br><h3 id="aws-step-functions-sf">AWS Step Functions (SF)</h3><ul><li>Build serverless visual workflow to orchestrate your Lambda functions</li><li>You write <strong>state machine</strong> in <strong>declarative JSON</strong>, you write a <strong>decider program</strong> to separate activity steps from decision steps.</li></ul><br><h3 id="aws-simple-workflow-service-swf">AWS Simple Workflow Service (SWF)</h3><ul><li>Code runs on EC2 (not Serverless)</li><li>Older service. Use SWF when you need external signal signals to intervene in the process or need the child process to pass value to the parent process, otherwise, use <strong>Step Functions</strong> for new applications.</li></ul><br><h3 id="aws-organization">AWS Organization</h3><ul><li>Global service to manage multiple AWS accounts e.g. accounts per department, per cost center, per environment (dev, test, prod)</li><li>Pricing benefits from <strong>aggregated usage across accounts</strong>.</li><li><strong>Consolidate billing</strong> across all accounts - single payment method</li><li>Organization has multiple <strong>Organization Units (OUs)</strong> (or accounts) based on department, cost center or environment, OU can have other OUs (hierarchy)</li><li>Organization has <strong>one master account</strong> and <strong>multiple member accounts</strong></li><li>You can apply <strong>Service Control Policies (SCPs)</strong> at OU or account level, SCP is applied to all users and roles in that account</li><li>SPC Deny take precedence over Allow in the full OU tree of an account e.g. allowed at the account level but deny at OU level is = deny</li><li>Master account can do anything even if you apply SCP</li><li>To merge Firm_A Organization with Firm_B Organization<ol><li>Remove all member accounts from Firm_A organization</li><li>Delete the Firm_A organization</li><li>Invite Firm_A master account to join Firm_B organization as a member account</li></ol></li><li><strong>AWS Resource Access Manager (RAM)</strong> helps you to create your AWS resources once, and securely share across accounts within OUs in AWS Organization. You can share <u>Transit Gateways, Subnets, AWS License Manager configurations, Route 53 resolver rules</u>, etc.</li><li>One account can share resources with another individual account within AWS organization with the help of <strong>RAM</strong>. You must enable resource sharing at AWS Organization level.</li><li><strong>AWS Control Tower</strong> integrated with AWS Organization helps you to <strong>quickly setup and configure a new AWS account</strong> with <strong>best practices</strong> from base called as <strong>landing zone</strong></li></ul><br><h3 id="aws-opsworks">AWS OpsWorks</h3><ul><li>Provide managed instances of <strong>Chef</strong> and <strong>Puppet</strong> configuration management services, which help to configure and operate applications in AWS.</li><li>Configuration as Code - OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across EC2 instances using Code.</li><li><strong>OpsWork Stack</strong> let you model your application as a stack containing different layers, such as load balancing, database, and application server.</li></ul><br><h3 id="aws-glue">AWS Glue</h3><ul><li>Serverless, fully managed ETL (extract, transform, and load) service</li><li><strong>AWS Glue Crawler</strong> scan data from data-source such as S3 or DynamoDB table, determine the schema for data, and then creates metadata tables in the AWS Glue Data Catalog.</li><li>AWS Glue provides <strong>classifiers</strong> for CSV, JSON, AVRO, XML or database to determine the schema for data</li></ul><h2 id="containers">Containers</h2><hr><ul><li><strong>ECR (Elastic Container Registry)</strong> is Docker Hub to pull and push Docker images, managed by Amazon.</li><li><strong>ECS (Elastic Container Service)</strong> ECS is a container management service to run, stop, and manage Docker containers on a cluster</li><li><strong>ECS Task Definition</strong> where you configure task and container definition<ul><li>Specify <strong>ECS Task IAM Role</strong> for ECS task (Docker container instance) to access AWS services like S3 bucket or DynamoDB</li><li>Specify <strong>Task Execution IAM Role</strong> i.e. <code>ecsTaskExecutionRole</code> for EC2 (ECS Agent) to pull docker images from ECR, make API calls to ECS service and publish container logs to Amazon CloudWatch on your behalf</li><li>Add container by specifying docker image, memory, port mappings, health-check, etc.</li></ul></li><li>You can create multiple ECS Task Definitions - e.g. one task definition to run a web application on the Nginx server and another task definition to run a microservice on Tomcat.</li><li><strong>ECS Service Definition</strong> where you configure cluster, ELB, ASG, task definition, and number of tasks to run multiple similar <strong>ECS Task</strong>, which deploys a docker container on EC2 instance. One EC2 instance can run multiple ECS tasks.</li><li><strong>Amazon EC2 Launch Type:</strong> You manage EC2 instances of ECS Cluster. You must install <strong>ECS Agent</strong> on each EC2 instance. Cheaper. Good for predictable, long-running tasks.</li><li><strong>ECS Agent</strong> The agent sends information about the EC2 instance’s current running tasks and resource utilization to Amazon ECS. It starts and stops tasks whenever it receives a request from Amazon ECS</li><li><strong>Fargate Launch Type:</strong> Serverless, EC2 instances are managed by Fargate. You only manage and pay for container resources. Costlier. Good for variable, short-running tasks</li><li><strong>EKS (Elastic Kubernetes Service)</strong> is managed Kubernetes clusters on AWS</li></ul><h2 id="cheat-sheet">Cheat Sheet</h2><hr><table><thead><tr><th style="text-align:left">AWS Service</th><th style="text-align:left">Keywords</th></tr></thead><tbody><tr><td style="text-align:left"><strong>Security</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left">Amazon CloudWatch</td><td style="text-align:left">Metrics, Logs, Alarms</td></tr><tr><td style="text-align:left">AWS CloudTrail</td><td style="text-align:left">Audit Events</td></tr><tr><td style="text-align:left">AWS WAF</td><td style="text-align:left">Firewall, SQL injection, Cross-site scripting (XSS), Layer 7 attacks</td></tr><tr><td style="text-align:left">AWS Shield</td><td style="text-align:left">DDoS attack, Layer 3 &amp; 4 attacks</td></tr><tr><td style="text-align:left">Amazon Macie</td><td style="text-align:left">Sensitive Data, Personally Identifiable Information (PII)</td></tr><tr><td style="text-align:left">Amazon Inspector</td><td style="text-align:left">EC2 Security Assessment, Unintended Network Accessibility</td></tr><tr><td style="text-align:left">Amazon GuardDuty</td><td style="text-align:left">Analyze VPC Flow Logs, Threat Detection</td></tr><tr><td style="text-align:left">AWS VPN</td><td style="text-align:left">Online Network Connection, Long-term Continuous transfer, Low to Moderate Bandwidth</td></tr><tr><td style="text-align:left">AWS Direct Connect</td><td style="text-align:left">Private Secure Dedicated Connection, Long-term Continuous transfer, High Bandwidth</td></tr><tr><td style="text-align:left"><strong>Application Integration</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left">Amazon SNS</td><td style="text-align:left">Serverless, PubSub, Fan-out</td></tr><tr><td style="text-align:left">Amazon SQS</td><td style="text-align:left">Serverless, Decoupled, Queue, Fan-out</td></tr><tr><td style="text-align:left">Amazon MQ</td><td style="text-align:left">ActiveMQ</td></tr><tr><td style="text-align:left">Amazon SWF</td><td style="text-align:left">Serverless, Simple Workflow Service, Decoupled, Task Coordinator, Distributed &amp; Background Jobs</td></tr><tr><td style="text-align:left">AWS Step Functions (SF)</td><td style="text-align:left">Orchestrate / Coordinate Lambda functions and ECS containers into a workflow</td></tr><tr><td style="text-align:left">AWS OpsWork</td><td style="text-align:left">Chef &amp; Puppet</td></tr><tr><td style="text-align:left"><strong>Storage</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left">EBS</td><td style="text-align:left">Block Storage Volume for EC2</td></tr><tr><td style="text-align:left">EFS</td><td style="text-align:left">Network File System for EC2, Concurrent access</td></tr><tr><td style="text-align:left">Amazon S3</td><td style="text-align:left">Serverless, Block Storage - Photos &amp; Videos, Website Hosting</td></tr><tr><td style="text-align:left">Amazon Athena</td><td style="text-align:left">Query data in S3 using SQL</td></tr><tr><td style="text-align:left">AWS Snow Family</td><td style="text-align:left">Offline Data Migration, Petabyte to exabyte Scale</td></tr><tr><td style="text-align:left">AWS DataSync</td><td style="text-align:left">Online Data Transfer, Immediate One-time transfer</td></tr><tr><td style="text-align:left">AWS Storage Gateway</td><td style="text-align:left">Hybrid Storage b/w On-premise and AWS</td></tr><tr><td style="text-align:left"><strong>Compute</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left">AWS Lambda</td><td style="text-align:left">Serverless, FaaS</td></tr><tr><td style="text-align:left"><strong>Database</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left">Amazon RDS</td><td style="text-align:left">Relational Database - PostgreSQL, MySQL, MariaDB, Oracle, and SQL Server</td></tr><tr><td style="text-align:left">Amazon Aurora</td><td style="text-align:left">Relational Database - Amazon-Owned</td></tr><tr><td style="text-align:left">Amazon DynamoDB</td><td style="text-align:left">Serverless, key-value NoSQL Database - Amazon-Owned</td></tr><tr><td style="text-align:left">Amazon DocumentDB</td><td style="text-align:left">Document Database, JSON documents - MongoDB</td></tr><tr><td style="text-align:left">Amazon Neptune</td><td style="text-align:left">Graph Database, Social Media Relationship</td></tr><tr><td style="text-align:left">Amazon Timestream</td><td style="text-align:left">Time Series Database</td></tr><tr><td style="text-align:left">Amazon Redshift</td><td style="text-align:left">Columnar Database, Analytics, BI, Parallel Query</td></tr><tr><td style="text-align:left">Amazon Elasticache</td><td style="text-align:left">Redis and Memcached, In-memory Cache</td></tr><tr><td style="text-align:left">Amazon EMR</td><td style="text-align:left">Elastic MapReduce, Big Data - Apache Hadoop, Spark, Hive, Hbase, Flink, Hudi</td></tr><tr><td style="text-align:left">Amazon Elasticsearch Service</td><td style="text-align:left">Elasticsearch, ELK</td></tr><tr><td style="text-align:left"><strong>Microservices</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left">Elastic Container Registry (ECR)</td><td style="text-align:left">Docker image repository, DockerHub</td></tr><tr><td style="text-align:left">Elastic Container Service (ECS)</td><td style="text-align:left">Docker container management system</td></tr><tr><td style="text-align:left">AWS Fargate</td><td style="text-align:left">Serverless ECS</td></tr><tr><td style="text-align:left">AWS X-Ray</td><td style="text-align:left">Trace Request, Debug</td></tr><tr><td style="text-align:left"><strong>Developer</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left">AWS CodeCommit</td><td style="text-align:left">like GitHub, Git-based Source Code Repository</td></tr><tr><td style="text-align:left">AWS CodeBuild</td><td style="text-align:left">like Jenkins CI, Code Compile, Build &amp; Test</td></tr><tr><td style="text-align:left">AWS CodeDeploy</td><td style="text-align:left">Code deployment to EC2, Fargate, and Lambda</td></tr><tr><td style="text-align:left">AWS CodePipeline</td><td style="text-align:left">CICD pipelines, Rapid Software or Build Release</td></tr><tr><td style="text-align:left">AWS CloudShell</td><td style="text-align:left">CLI, Browser-based Shell</td></tr><tr><td style="text-align:left">AWS Elastic Beanstalk</td><td style="text-align:left">PaaS, Quick deploy applications - Java-Tomcat, PHP/Python-Apache HTTP Server, Node.js-Nginx</td></tr><tr><td style="text-align:left">Amazon Workspaces</td><td style="text-align:left">Desktop-as-a-Service, Virtual Windows or Linux Desktops</td></tr><tr><td style="text-align:left">Amazon AppStream 2.0</td><td style="text-align:left">Install Applications on Virtual Desktop and access it from Mobile, Tab or Remote Desktop through Browser</td></tr><tr><td style="text-align:left">AWS CloudFormation</td><td style="text-align:left">Infrastructure as Code, Replicate Infrastructure</td></tr><tr><td style="text-align:left">AWS Certificate Manager (ACM)</td><td style="text-align:left">Create, renew, deploy SSL/TLS certificates to CloudFront and ELB</td></tr><tr><td style="text-align:left">AWS Migration Hub</td><td style="text-align:left">Centralized Tracking on the progress of all migrations across AWS</td></tr><tr><td style="text-align:left">AWS Glue</td><td style="text-align:left">Data ETL (extract, transform, load), Crawler, Data Catalogue</td></tr><tr><td style="text-align:left">AWS AppSync</td><td style="text-align:left">GraphQL</td></tr><tr><td style="text-align:left">Amazon Elastic Transcoder</td><td style="text-align:left">Media (Audio, Video) converter</td></tr></tbody></table><br><h6 id="important-ports">Important Ports</h6><table><thead><tr><th style="text-align:left">Protocol/Database</th><th style="text-align:left">Port</th></tr></thead><tbody><tr><td style="text-align:left">FTP</td><td style="text-align:left">21</td></tr><tr><td style="text-align:left">SSH</td><td style="text-align:left">22</td></tr><tr><td style="text-align:left">SFTP</td><td style="text-align:left">22</td></tr><tr><td style="text-align:left">HTTP</td><td style="text-align:left">80</td></tr><tr><td style="text-align:left">HTTPS</td><td style="text-align:left">443</td></tr><tr><td style="text-align:left">RDP</td><td style="text-align:left">3389</td></tr><tr><td style="text-align:left">NFS</td><td style="text-align:left">2049</td></tr><tr><td style="text-align:left">PostgresSQL</td><td style="text-align:left">5432</td></tr><tr><td style="text-align:left">MySQL</td><td style="text-align:left">3306</td></tr><tr><td style="text-align:left">MariaDB</td><td style="text-align:left">3306</td></tr><tr><td style="text-align:left">Aurora</td><td style="text-align:left">3306 or 5432</td></tr><tr><td style="text-align:left">Oracle RDS</td><td style="text-align:left">1521</td></tr><tr><td style="text-align:left">MSSQL Server</td><td style="text-align:left">1433</td></tr></tbody></table><h2 id="white-papers">White Papers</h2><hr><h3 id="disaster-recovery">Disaster Recovery</h3><ol><li>RPO - Recovery Point Objective - How much data is lost to recover from a disaster e.g. last 20 min data lost before the disaster</li><li>RTO - Recovery Time Objective - How much downtime require to recover from a disaster e.g. 1-hour downtime to start disaster recovery service</li><li>Disaster Recovery techniques (RPO &amp; RTO reduces and the cost goes up as we go down)<ul><li><strong>Backup &amp; Restore</strong> – Data is backed up and restored, with nothing running</li><li><strong>Pilot light</strong> – Only minimal critical service like RDS is running and the rest of the services can be recreated and scaled during recovery</li><li><strong>Warm Standby</strong> – Fully functional site with minimal configuration is available and can be scaled during recovery</li><li><strong>Multi-Site</strong> – Fully functional site with identical configuration is available and processes the load</li></ul></li><li>Use <strong>Amazon Aurora Global Database</strong> for RDS and <strong>DynamoDB Global Table</strong> for NoSQL databases for disaster recovery with stringent RPO of 1 second and RTO of 1 minute.</li></ol><br><h3 id="5-pillars-of-the-aws-well-architected-framework">5 Pillars of the AWS Well-Architected Framework</h3><p>The 5 Pillars of <a href="https://aws.amazon.com/blogs/apn/the-5-pillars-of-the-aws-well-architected-framework/">AWS Well-Architected Framework</a> are as follows:-</p><ol><li><strong>Operational Excellence</strong><ul><li>Use <strong>AWS Trusted Advisor</strong> to get recommendations on AWS best practices, optimize AWS infrastructure, improve security and performance, reduce costs, and monitor service quotas</li><li>Use <strong>Serverless application</strong> API Gateway (Front layer for auth, cache, routing), Lambda (Compute), DynamoDB (Database), DAX (Caching), S3 (File Storage) and Cognito User Pools (Auth), CloudFront (Deliver content globally), SES (Send email), SQS &amp; SNS (Publish &amp; Notify events)</li></ul></li><li><strong>Security</strong><ul><li>Use <strong>AWS Shield</strong> and <strong>AWS WAF</strong> to prevent network, transport and application layer security attacks</li></ul></li><li><strong>Reliability</strong></li><li><strong>Performance Efficiency</strong></li><li><strong>Cost Optimization</strong><ul><li>Use <strong>AWS Cost Explorer</strong> to forecast daily or monthly cloud costs based on ML applied to your historical cost</li><li>Use <strong>AWS Budget</strong> to set yearly, quarterly, monthly, daily or fixed cost or usage budget for AWS services and get notified when actual or forecast cost or usage exceeds budget limit.</li><li>Use <strong>AWS Saving Plans</strong> to get a discount in exchange for usage commitment e.g. $10/hour for one-year or three-year period. AWS offers three types of Savings Plans – <strong>1. Compute Savings Plans</strong> apply to usage across Amazon EC2, AWS Lambda, and AWS Fargate. <strong>2. EC2 Instance Savings Plans</strong> apply to EC2 usage, and <strong>3. SageMaker Savings Plans</strong> apply to SageMaker usage.</li><li>Use <strong>VPC Gateway endpoint</strong> to access S3 and DynamoDB privately within AWS network to reduce data transfer cost</li><li>Use <strong>AWS Organization</strong> for consolidated billing and aggregated usage benefits across AWS accounts</li></ul></li></ol><h2 id="disclaimer">Disclaimer</h2><hr><p>I have created the exam notes after watching many training videos and solving tons of practice exam questions. I found that some information given in training videos and practice exams were not correct (or should say not updated). Amazon AWS is growing very fast, they keep enhancing their services with loads of new features as well as introducing new AWS services.</p><p>I have personally verified each and every statement in this exam notes from AWS services documentation and FAQs at the time of writing these notes. Please comment and share if you find any statement has become stale or irrelevant after updates in AWS services. Let’s make this exam notes helpful and trustful for all AWS aspirants!</p></div><footer class="post__footer"><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"></path></svg><ul class="tags__list"><li class="tags__item"><a class="tags__link btn" href="/tags/aws/" rel="tag">AWS</a></li><li class="tags__item"><a class="tags__link btn" href="/tags/certification/" rel="tag">Certification</a></li><li class="tags__item"><a class="tags__link btn" href="/tags/popular-posts/" rel="tag">Popular Posts</a></li></ul></div></footer></div></div>